{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa10769c",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3026f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import warnings\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import your models\n",
    "from src.models.text_model import create_text_model\n",
    "from src.models.multimodal_model import create_multimodal_model\n",
    "\n",
    "# Setup plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"  Setup complete!\")\n",
    "print(f\"  Project root: {project_root}\")\n",
    "print(f\"  Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d522522",
   "metadata": {},
   "source": [
    "## ü§ñ Load Your Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAPredictor:\n",
    "    \"\"\"Easy-to-use VQA prediction interface\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load config\n",
    "        with open('config.yaml', 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # Load answer vocabulary\n",
    "        with open('answers.txt', 'r', encoding='utf-8') as f:\n",
    "            answers = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        self.answers = sorted(list(set(answers)))\n",
    "        self.answer_to_idx = {ans: idx for idx, ans in enumerate(self.answers)}\n",
    "        self.idx_to_answer = {idx: ans for ans, idx in self.answer_to_idx.items()}\n",
    "        self.num_classes = len(self.answers)\n",
    "        \n",
    "        # Simple vocab for inference (you can expand this)\n",
    "        self.vocab = {\n",
    "            '<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3,\n",
    "            'what': 4, 'is': 5, 'the': 6, 'in': 7, 'of': 8, 'a': 9, 'and': 10,\n",
    "            'to': 11, 'this': 12, 'image': 13, 'shown': 14, 'visible': 15,\n",
    "            'organ': 16, 'tissue': 17, 'cell': 18, 'structure': 19, 'abnormal': 20,\n",
    "            'normal': 21, 'pathology': 22, 'medical': 23, 'diagnosis': 24, 'disease': 25\n",
    "        }\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Load models\n",
    "        self._load_models()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"  VQA Predictor initialized!\")\n",
    "        print(f\"  Device: {self.device}\")\n",
    "        print(f\"  Answer classes: {self.num_classes}\")\n",
    "        print(f\"  Vocabulary size: {self.vocab_size}\")\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load both trained models\"\"\"\n",
    "        # Text-only model\n",
    "        self.text_model = create_text_model(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embedding_dim=self.config['text']['embedding_dim'],\n",
    "            hidden_dim=self.config['model']['baseline']['hidden_dim'],\n",
    "            num_classes=self.num_classes,\n",
    "            dropout=self.config['model']['baseline']['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Multimodal model  \n",
    "        self.multimodal_model = create_multimodal_model(\n",
    "            model_type='concat',\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embedding_dim=self.config['text']['embedding_dim'],\n",
    "            text_hidden_dim=self.config['model']['baseline']['hidden_dim'],\n",
    "            fusion_hidden_dim=self.config['model']['baseline']['hidden_dim'],\n",
    "            dropout=self.config['model']['baseline']['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Load checkpoints\n",
    "        try:\n",
    "            text_checkpoint = torch.load(\n",
    "                'checkpoints/text_baseline_lstm_notebook/best_model.pth', \n",
    "                map_location=self.device\n",
    "            )\n",
    "            multimodal_checkpoint = torch.load(\n",
    "                'checkpoints/multimodal_concat/best_model.pth',\n",
    "                map_location=self.device\n",
    "            )\n",
    "            \n",
    "            self.text_model.load_state_dict(text_checkpoint)\n",
    "            self.multimodal_model.load_state_dict(multimodal_checkpoint)\n",
    "            \n",
    "            print(\"    Text-only model loaded\")\n",
    "            print(\"    Multimodal model loaded\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"    Error loading models: {e}\")\n",
    "            print(\"  Make sure you have trained both models first!\")\n",
    "            return\n",
    "        \n",
    "        self.text_model.eval()\n",
    "        self.multimodal_model.eval()\n",
    "    \n",
    "    def encode_question(self, question: str, max_length: int = 32):\n",
    "        \"\"\"Encode question to tensor\"\"\"\n",
    "        words = question.lower().split()\n",
    "        indices = [self.vocab.get(word, self.vocab.get('<UNK>', 1)) for word in words]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indices) < max_length:\n",
    "            indices = indices + [self.vocab.get('<PAD>', 0)] * (max_length - len(indices))\n",
    "        else:\n",
    "            indices = indices[:max_length]\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "    \n",
    "    def predict(self, image_path=None, question=\"\", top_k=5):\n",
    "        \"\"\"Make prediction with both models\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Encode question\n",
    "            question_tensor = self.encode_question(question)\n",
    "            \n",
    "            # Text-only prediction\n",
    "            text_logits = self.text_model(question_tensor)\n",
    "            text_probs = F.softmax(text_logits, dim=1)\n",
    "            text_top_k = torch.topk(text_probs, top_k, dim=1)\n",
    "            \n",
    "            text_results = []\n",
    "            for prob, idx in zip(text_top_k.values[0], text_top_k.indices[0]):\n",
    "                text_results.append({\n",
    "                    'answer': self.idx_to_answer[idx.item()],\n",
    "                    'confidence': prob.item()\n",
    "                })\n",
    "            \n",
    "            results = {\n",
    "                'question': question,\n",
    "                'text_only': text_results\n",
    "            }\n",
    "            \n",
    "            # Multimodal prediction (if image provided)\n",
    "            if image_path and Path(image_path).exists():\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    image_tensor = self.image_transform(image).unsqueeze(0).to(self.device)\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    mm_logits = self.multimodal_model(question_tensor, image_tensor)\n",
    "                    mm_probs = F.softmax(mm_logits, dim=1)\n",
    "                    mm_top_k = torch.topk(mm_probs, top_k, dim=1)\n",
    "                    \n",
    "                    mm_results = []\n",
    "                    for prob, idx in zip(mm_top_k.values[0], mm_top_k.indices[0]):\n",
    "                        mm_results.append({\n",
    "                            'answer': self.idx_to_answer[idx.item()],\n",
    "                            'confidence': prob.item()\n",
    "                        })\n",
    "                    \n",
    "                    results['multimodal'] = mm_results\n",
    "                    results['image'] = image\n",
    "                    results['image_path'] = image_path\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {image_path}: {e}\")\n",
    "            \n",
    "            return results\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = VQAPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b0ba6",
   "metadata": {},
   "source": [
    "## üéØ Quick Test - Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840faa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(image_path, question, top_k=3):\n",
    "    \"\"\"Quick test function with visualization\"\"\"\n",
    "    result = predictor.predict(image_path, question, top_k=top_k)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Display image if available\n",
    "    if 'image' in result:\n",
    "        # Image subplot\n",
    "        ax1 = plt.subplot(1, 3, 1)\n",
    "        plt.imshow(result['image'])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image: {Path(image_path).name}\", fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Text predictions subplot\n",
    "        ax2 = plt.subplot(1, 3, 2)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        text_content = f\"Question:\\n{question}\\n\\n\"\n",
    "        text_content += \"  Text-only predictions:\\n\"\n",
    "        for i, pred in enumerate(result['text_only'][:top_k]):\n",
    "            text_content += f\"{i+1}. {pred['answer']} ({pred['confidence']:.3f})\\n\"\n",
    "        \n",
    "        plt.text(0.05, 0.95, text_content, fontsize=11, verticalalignment='top',\n",
    "                transform=ax2.transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "        \n",
    "        # Multimodal predictions subplot\n",
    "        ax3 = plt.subplot(1, 3, 3)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if 'multimodal' in result:\n",
    "            mm_content = \"   Multimodal predictions:\\n\"\n",
    "            for i, pred in enumerate(result['multimodal'][:top_k]):\n",
    "                mm_content += f\"{i+1}. {pred['answer']} ({pred['confidence']:.3f})\\n\"\n",
    "            \n",
    "            # Highlight if different\n",
    "            text_answer = result['text_only'][0]['answer']\n",
    "            mm_answer = result['multimodal'][0]['answer']\n",
    "            \n",
    "            if text_answer != mm_answer:\n",
    "                mm_content += \"\\n  Models disagree!\\n\"\n",
    "                mm_content += f\"Text: {text_answer}\\n\"\n",
    "                mm_content += f\"Visual: {mm_answer}\"\n",
    "                box_color = \"salmon\"\n",
    "            else:\n",
    "                mm_content += f\"\\n  Both agree: {text_answer}\"\n",
    "                box_color = \"lightgreen\"\n",
    "        else:\n",
    "            mm_content = \"  No image provided\\nOnly text prediction shown\"\n",
    "            box_color = \"lightyellow\"\n",
    "        \n",
    "        plt.text(0.05, 0.95, mm_content, fontsize=11, verticalalignment='top',\n",
    "                transform=ax3.transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=box_color, alpha=0.5))\n",
    "    \n",
    "    else:\n",
    "        # Text-only mode\n",
    "        plt.axis('off')\n",
    "        content = f\"Question: {question}\\n\\n\"\n",
    "        content += \"  Text-only predictions:\\n\"\n",
    "        for i, pred in enumerate(result['text_only'][:top_k]):\n",
    "            content += f\"  {i+1}. {pred['answer']} ({pred['confidence']:.3f})\\n\"\n",
    "        content += \"\\n  No image provided\"\n",
    "        \n",
    "        plt.text(0.1, 0.5, content, fontsize=12, verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage - modify paths as needed\n",
    "print(\"  Testing your models!\")\n",
    "print(\"Modify the paths below to test with your images:\")\n",
    "\n",
    "# Test with sample question\n",
    "sample_result = quick_test(\n",
    "    image_path=None,  # Change to your image path, e.g., \"data/train/image1.png\"\n",
    "    question=\"What organ is shown in the image?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918117b9",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Test with Real Image\n",
    "\n",
    "**Replace the path below with one of your PathVQA images!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe587e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List some available images\n",
    "image_dir = Path(\"data/train\")\n",
    "if image_dir.exists():\n",
    "    images = list(image_dir.glob(\"*.png\"))[:10]\n",
    "    print(f\"Available images in {image_dir}:\")\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"  {i+1}. {img.name}\")\n",
    "    \n",
    "    if images:\n",
    "        # Test with first available image\n",
    "        sample_image = str(images[0])\n",
    "        print(f\"\\nTesting with: {sample_image}\")\n",
    "        \n",
    "        result = quick_test(\n",
    "            image_path=sample_image,\n",
    "            question=\"What is the primary structure visible in this pathology image?\"\n",
    "        )\n",
    "else:\n",
    "    print(\"  Image directory not found. Make sure 'data/train' exists with .png files.\")\n",
    "    print(\"You can still test with text-only predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c065307",
   "metadata": {},
   "source": [
    "## üî¨ Interactive Testing\n",
    "\n",
    "**Try different questions and images!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4dcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify these variables and re-run to test different combinations\n",
    "\n",
    "# Example questions you can try:\n",
    "questions = [\n",
    "    \"What organ is shown in the image?\",\n",
    "    \"What type of tissue is visible?\",\n",
    "    \"Is there any abnormality present?\",\n",
    "    \"What is the primary pathological finding?\",\n",
    "    \"What structures are visible in this sample?\"\n",
    "]\n",
    "\n",
    "print(\"  Try different questions:\")\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"{i+1}. {q}\")\n",
    "\n",
    "# Customize your test here:\n",
    "your_question = \"What organ is shown in the image?\"  # ‚Üê Change this\n",
    "your_image = None  # ‚Üê Change to your image path, e.g., \"data/train/your_image.png\"\n",
    "\n",
    "print(f\"\\n  Testing with: '{your_question}'\")\n",
    "custom_result = quick_test(your_image, your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9d917",
   "metadata": {},
   "source": [
    "## üìä Batch Evaluation\n",
    "\n",
    "**Evaluate your models on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(test_csv=\"testrenamed.csv\", image_dir=\"data/train\", num_samples=100):\n",
    "    \"\"\"Evaluate both models on test set\"\"\"\n",
    "    \n",
    "    if not Path(test_csv).exists():\n",
    "        print(f\"  Test file {test_csv} not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(test_csv).head(num_samples)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"  Evaluating on {len(df)} test samples...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        # Construct image path\n",
    "        image_name = row['image']\n",
    "        if not str(image_name).endswith('.png'):\n",
    "            image_name = f\"{image_name}.png\"\n",
    "        \n",
    "        image_path = Path(image_dir) / image_name\n",
    "        question = row['question']\n",
    "        true_answer = row['answer']\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = predictor.predict(\n",
    "            str(image_path) if image_path.exists() else None, \n",
    "            question, \n",
    "            top_k=1\n",
    "        )\n",
    "        \n",
    "        text_pred = pred['text_only'][0]['answer']\n",
    "        text_conf = pred['text_only'][0]['confidence']\n",
    "        \n",
    "        result = {\n",
    "            'question': question,\n",
    "            'true_answer': true_answer,\n",
    "            'text_prediction': text_pred,\n",
    "            'text_confidence': text_conf,\n",
    "            'text_correct': text_pred.lower() == true_answer.lower(),\n",
    "            'image_path': str(image_path) if image_path.exists() else None\n",
    "        }\n",
    "        \n",
    "        if 'multimodal' in pred:\n",
    "            mm_pred = pred['multimodal'][0]['answer']\n",
    "            mm_conf = pred['multimodal'][0]['confidence']\n",
    "            result.update({\n",
    "                'multimodal_prediction': mm_pred,\n",
    "                'multimodal_confidence': mm_conf,\n",
    "                'multimodal_correct': mm_pred.lower() == true_answer.lower(),\n",
    "                'models_agree': text_pred.lower() == mm_pred.lower()\n",
    "            })\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    text_acc = results_df['text_correct'].mean()\n",
    "    print(f\"\\n  Results:\")\n",
    "    print(f\"  Text-only accuracy: {text_acc:.4f} ({text_acc*100:.2f}%)\")\n",
    "    \n",
    "    if 'multimodal_correct' in results_df.columns:\n",
    "        mm_acc = results_df['multimodal_correct'].mean()\n",
    "        improvement = mm_acc - text_acc\n",
    "        agreement = results_df['models_agree'].mean()\n",
    "        \n",
    "        print(f\"  Multimodal accuracy: {mm_acc:.4f} ({mm_acc*100:.2f}%)\")\n",
    "        print(f\"  Improvement: {improvement:.4f} ({improvement*100:.2f} pp)\")\n",
    "        print(f\"  Model agreement: {agreement:.4f} ({agreement*100:.1f}%)\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"    Multimodal model is better!\")\n",
    "        else:\n",
    "            print(f\"    Text-only model performed better on this sample\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run evaluation\n",
    "print(\"  Starting batch evaluation...\")\n",
    "eval_results = evaluate_batch(num_samples=50)  # Start with 50 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635525b",
   "metadata": {},
   "source": [
    "## üìà Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_results is not None and len(eval_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    text_acc = eval_results['text_correct'].mean()\n",
    "    \n",
    "    if 'multimodal_correct' in eval_results.columns:\n",
    "        mm_acc = eval_results['multimodal_correct'].mean()\n",
    "        accuracies = [text_acc, mm_acc]\n",
    "        labels = ['Text-only', 'Multimodal']\n",
    "        colors = ['skyblue', 'lightcoral']\n",
    "    else:\n",
    "        accuracies = [text_acc]\n",
    "        labels = ['Text-only']\n",
    "        colors = ['skyblue']\n",
    "    \n",
    "    bars = axes[0,0].bar(labels, accuracies, color=colors)\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].set_title('Model Accuracy Comparison')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, acc + 0.02, \n",
    "                      f'{acc:.3f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Confidence distribution\n",
    "    axes[0,1].hist(eval_results['text_confidence'], alpha=0.6, label='Text-only', \n",
    "                   bins=20, color='skyblue', edgecolor='black')\n",
    "    if 'multimodal_confidence' in eval_results.columns:\n",
    "        axes[0,1].hist(eval_results['multimodal_confidence'], alpha=0.6, label='Multimodal', \n",
    "                       bins=20, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_xlabel('Confidence Score')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].set_title('Prediction Confidence Distribution')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Agreement analysis (if multimodal available)\n",
    "    if 'models_agree' in eval_results.columns:\n",
    "        agreement = eval_results['models_agree'].mean()\n",
    "        sizes = [agreement, 1-agreement]\n",
    "        labels_pie = [f'Agree\\n({agreement:.1%})', f'Disagree\\n({1-agreement:.1%})']\n",
    "        colors_pie = ['lightgreen', 'salmon']\n",
    "        \n",
    "        axes[1,0].pie(sizes, labels=labels_pie, colors=colors_pie, autopct='', startangle=90)\n",
    "        axes[1,0].set_title('Model Agreement Analysis')\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'Multimodal results\\nnot available', \n",
    "                       ha='center', va='center', fontsize=12)\n",
    "        axes[1,0].set_title('Model Agreement')\n",
    "    \n",
    "    # 4. Performance by confidence\n",
    "    # Bin predictions by confidence and show accuracy\n",
    "    eval_results['conf_bin'] = pd.cut(eval_results['text_confidence'], bins=5)\n",
    "    conf_acc = eval_results.groupby('conf_bin')['text_correct'].mean()\n",
    "    \n",
    "    x_pos = range(len(conf_acc))\n",
    "    axes[1,1].bar(x_pos, conf_acc.values, color='steelblue', alpha=0.7)\n",
    "    axes[1,1].set_xlabel('Confidence Bins')\n",
    "    axes[1,1].set_ylabel('Accuracy')\n",
    "    axes[1,1].set_title('Accuracy vs Confidence (Text-only)')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].set_xticklabels([f'{interval.left:.2f}-{interval.right:.2f}' \n",
    "                               for interval in conf_acc.index], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"  Analysis complete! See visualizations above.\")\n",
    "else:\n",
    "    print(\"  No evaluation results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705e335",
   "metadata": {},
   "source": [
    "## üîç Analyze Specific Examples\n",
    "\n",
    "**Look at interesting cases where models agree/disagree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319607d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_interesting_examples(results_df, example_type='disagreement', n_examples=3):\n",
    "    \"\"\"Show specific types of examples\"\"\"\n",
    "    \n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\" No results available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"  Showing {example_type} examples:\\n\")\n",
    "    \n",
    "    if example_type == 'disagreement' and 'multimodal_prediction' in results_df.columns:\n",
    "        # Cases where models disagree\n",
    "        disagreements = results_df[results_df['models_agree'] == False]\n",
    "        \n",
    "        if len(disagreements) == 0:\n",
    "            print(\" No disagreements found - models always agree!\")\n",
    "            return\n",
    "        \n",
    "        sample = disagreements.sample(min(n_examples, len(disagreements)))\n",
    "        \n",
    "        for idx, (_, row) in enumerate(sample.iterrows()):\n",
    "            print(f\"Example {idx+1}:\")\n",
    "            print(f\"  Question: {row['question']}\")\n",
    "            print(f\"  True answer: {row['true_answer']}\")\n",
    "            print(f\"  Text-only: {row['text_prediction']} {' ' if row['text_correct'] else ' '}\")\n",
    "            print(f\"  Multimodal: {row['multimodal_prediction']} {' ' if row['multimodal_correct'] else ' '}\")\n",
    "            \n",
    "            # Determine which is correct\n",
    "            if row['text_correct'] and not row['multimodal_correct']:\n",
    "                print(f\"  ‚Üí Text-only was right!  \")\n",
    "            elif row['multimodal_correct'] and not row['text_correct']:\n",
    "                print(f\"  ‚Üí Multimodal was right!   \")\n",
    "            elif row['text_correct'] and row['multimodal_correct']:\n",
    "                print(f\"  ‚Üí Both wrong but disagreed on how  \")\n",
    "            else:\n",
    "                print(f\"  ‚Üí Both were wrong  \")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    elif example_type == 'correct':\n",
    "        # Cases where model(s) are correct\n",
    "        if 'multimodal_correct' in results_df.columns:\n",
    "            correct_cases = results_df[\n",
    "                (results_df['text_correct'] == True) | (results_df['multimodal_correct'] == True)\n",
    "            ]\n",
    "        else:\n",
    "            correct_cases = results_df[results_df['text_correct'] == True]\n",
    "        \n",
    "        sample = correct_cases.sample(min(n_examples, len(correct_cases)))\n",
    "        \n",
    "        for idx, (_, row) in enumerate(sample.iterrows()):\n",
    "            print(f\"Example {idx+1}:\")\n",
    "            print(f\"  Question: {row['question']}\")\n",
    "            print(f\"  Correct answer: {row['true_answer']}  \")\n",
    "            print(f\"  Text prediction: {row['text_prediction']}\")\n",
    "            if 'multimodal_prediction' in row:\n",
    "                print(f\"  Multimodal prediction: {row['multimodal_prediction']}\")\n",
    "            print()\n",
    "    \n",
    "    elif example_type == 'errors':\n",
    "        # Cases where models are wrong\n",
    "        if 'multimodal_correct' in results_df.columns:\n",
    "            error_cases = results_df[\n",
    "                (results_df['text_correct'] == False) & (results_df['multimodal_correct'] == False)\n",
    "            ]\n",
    "        else:\n",
    "            error_cases = results_df[results_df['text_correct'] == False]\n",
    "        \n",
    "        sample = error_cases.sample(min(n_examples, len(error_cases)))\n",
    "        \n",
    "        for idx, (_, row) in enumerate(sample.iterrows()):\n",
    "            print(f\"Example {idx+1}:\")\n",
    "            print(f\"  Question: {row['question']}\")\n",
    "            print(f\"  Correct answer: {row['true_answer']}\")\n",
    "            print(f\"  Text prediction: {row['text_prediction']}  \")\n",
    "            if 'multimodal_prediction' in row:\n",
    "                print(f\"  Multimodal prediction: {row['multimodal_prediction']}  \")\n",
    "            print()\n",
    "\n",
    "# Show different types of examples\n",
    "if eval_results is not None:\n",
    "    show_interesting_examples(eval_results, 'disagreement', 3)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    show_interesting_examples(eval_results, 'correct', 2)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    show_interesting_examples(eval_results, 'errors', 2)\n",
    "else:\n",
    "    print(\"Run the batch evaluation first to see examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56616951",
   "metadata": {},
   "source": [
    "##  Save Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438269d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_results is not None:\n",
    "    # Save results to CSV\n",
    "    output_file = f\"vqa_evaluation_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    eval_results.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"  Results saved to: {output_file}\")\n",
    "    print(f\"   Samples evaluated: {len(eval_results)}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'total_samples': len(eval_results),\n",
    "        'text_accuracy': eval_results['text_correct'].mean(),\n",
    "        'text_avg_confidence': eval_results['text_confidence'].mean()\n",
    "    }\n",
    "    \n",
    "    if 'multimodal_correct' in eval_results.columns:\n",
    "        summary.update({\n",
    "            'multimodal_accuracy': eval_results['multimodal_correct'].mean(),\n",
    "            'multimodal_avg_confidence': eval_results['multimodal_confidence'].mean(),\n",
    "            'improvement': eval_results['multimodal_correct'].mean() - eval_results['text_correct'].mean(),\n",
    "            'agreement_rate': eval_results['models_agree'].mean()\n",
    "        })\n",
    "    \n",
    "    print(\"\\n  Final Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        if 'accuracy' in key or 'improvement' in key or 'agreement' in key:\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {value:.4f} ({value*100:.2f}%)\")\n",
    "        elif 'confidence' in key:\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "else:\n",
    "    print(\"No results to save. Run the evaluation first!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
