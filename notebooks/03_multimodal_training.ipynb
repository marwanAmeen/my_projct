{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e2851b",
   "metadata": {},
   "source": [
    "# Multimodal VQA Training\n",
    "\n",
    "**Goal**: Train a Visual Question Answering model combining text + vision\n",
    "\n",
    "This notebook walks through:\n",
    "1. Loading images and questions together\n",
    "2. Building a multimodal model (LSTM + ResNet CNN)\n",
    "3. Training with fusion strategies\n",
    "4. Evaluating performance\n",
    "5. Comparing with text-only baseline (47%)\n",
    "\n",
    "**Target**: 60-70% accuracy with vision!\n",
    "\n",
    "**Note**: Can run on Google Colab (GPU recommended for faster training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531ff86",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "**For Google Colab**: This cell will automatically mount your Drive and install packages.\n",
    "**For Local**: This cell will skip Colab-specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf844d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Running locally\n"
     ]
    }
   ],
   "source": [
    "# Google Colab setup (auto-detects environment)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install packages\n",
    "    print(\"Installing packages...\")\n",
    "    !pip install -q torch torchvision tqdm pyyaml scikit-learn pandas matplotlib seaborn Pillow\n",
    "    \n",
    "    # Set project path\n",
    "    import os\n",
    "    PROJECT_PATH = \"/content/drive/MyDrive/WOA7015 Advanced Machine Learning/data\"\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"‚úì Running on Colab - Path: {PROJECT_PATH}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Running locally\n",
    "    PROJECT_PATH = None\n",
    "    print(\"‚úì Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad218ef4",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8500d09b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m     project_root = Path().absolute().parent\n\u001b[32m     11\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "if 'PROJECT_PATH' in globals() and PROJECT_PATH:\n",
    "    project_root = Path(PROJECT_PATH)\n",
    "else:\n",
    "    project_root = Path().absolute().parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataset import create_multimodal_dataloaders\n",
    "from src.models.multimodal_model import create_multimodal_model\n",
    "from src.training.multimodal_trainer import MultimodalVQATrainer\n",
    "from src.evaluation.metrics import VQAMetrics, calculate_accuracy\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules (important after fixing dataset.py)\n",
    "import importlib\n",
    "if 'src.data.dataset' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.data.dataset'])\n",
    "if 'src.models.multimodal_model' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.models.multimodal_model'])\n",
    "if 'src.training.multimodal_trainer' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.training.multimodal_trainer'])\n",
    "\n",
    "print(\"‚úì Modules reloaded - dataset.py changes applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0aa74",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6103398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Update config for multimodal training\n",
    "config['training']['batch_size'] = 16  # Smaller batch for multimodal\n",
    "config['training']['num_epochs'] = 10\n",
    "config['training']['learning_rate'] = 1e-4  # Lower LR for vision features\n",
    "config['model']['vision_encoder'] = 'resnet50'\n",
    "config['model']['fusion_strategy'] = 'concatenation'  # Start with simplest\n",
    "\n",
    "print(\"\\nMultimodal training config:\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Vision encoder: {config['model']['vision_encoder']}\")\n",
    "print(f\"  Fusion strategy: {config['model']['fusion_strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beaea4d",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Loading the PathVQA dataset with **images** using the new `MultimodalVQADataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4fc929",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create multimodal dataloaders\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset_path = \u001b[43mproject_root\u001b[49m / \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m train_loader, val_loader, test_loader, vocab_size, num_classes, vocab, answer_to_idx = create_multimodal_dataloaders(\n\u001b[32m      4\u001b[39m     train_csv=\u001b[38;5;28mstr\u001b[39m(dataset_path / \u001b[33m'\u001b[39m\u001b[33mtrainrenamed.csv\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m      5\u001b[39m     test_csv=\u001b[38;5;28mstr\u001b[39m(dataset_path / \u001b[33m'\u001b[39m\u001b[33mtestrenamed.csv\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     image_size=\u001b[32m224\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Data loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'project_root' is not defined"
     ]
    }
   ],
   "source": [
    "# Create multimodal dataloaders\n",
    "dataset_path = project_root / 'data'\n",
    "train_loader, val_loader, test_loader, vocab_size, num_classes, vocab, answer_to_idx = create_multimodal_dataloaders(\n",
    "    train_csv=str(dataset_path / 'trainrenamed.csv'),\n",
    "    test_csv=str(dataset_path / 'testrenamed.csv'),\n",
    "    image_dir=str(dataset_path / 'train'),\n",
    "    answers_file=str(dataset_path / 'answers.txt'),  # This was missing!\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    val_split=0.1,\n",
    "    num_workers=0,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "print(f\"  Data loaded successfully\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Questions: {sample_batch['question'].shape}\")\n",
    "print(f\"  Images: {sample_batch['image'].shape}\")  # [batch, 3, 224, 224]\n",
    "print(f\"  Answers: {sample_batch['answer'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check answer vocabulary consistency\n",
    "print(\"Answer vocabulary debug:\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Max answer index in answer_to_idx: {max(answer_to_idx.values()) if answer_to_idx else 'N/A'}\")\n",
    "print(f\"  '<UNK>' token index: {answer_to_idx.get('<UNK>', 'Not present')}\")\n",
    "\n",
    "# Check a few samples from the training data\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch answer indices:\")\n",
    "print(f\"  Min answer index: {sample_batch['answer'].min().item()}\")\n",
    "print(f\"  Max answer index: {sample_batch['answer'].max().item()}\")\n",
    "print(f\"  Answer indices range should be 0 to {num_classes-1}\")\n",
    "\n",
    "# Verify all indices are within valid range\n",
    "max_answer_in_batch = sample_batch['answer'].max().item()\n",
    "if max_answer_in_batch >= num_classes:\n",
    "    print(f\"‚ùå ERROR: Found answer index {max_answer_in_batch} but only {num_classes} classes!\")\n",
    "    print(\"This will cause the IndexError during training\")\n",
    "else:\n",
    "    print(\"‚úì All answer indices are within valid range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b14a02",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Images\n",
    "\n",
    "Let's verify the images are loading correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285397ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 4 sample images with their questions and answers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Get batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for i in range(4):\n",
    "    # Denormalize image for display\n",
    "    img = batch['image'][i].cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Get question and answer\n",
    "    question = batch['question'][i].cpu().numpy()\n",
    "    answer_idx = batch['answer'][i].item()\n",
    "    \n",
    "    # Decode question (first few words)\n",
    "    question_text = ' '.join([vocab.idx_to_word.get(idx, '<UNK>') \n",
    "                               for idx in question[:15] if idx > 0])\n",
    "    answer_text = vocab.idx_to_word.get(answer_idx, '<UNK>')\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Q: {question_text}...\\nA: {answer_text}\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Images loading correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e186db2",
   "metadata": {},
   "source": [
    "## 5. Create Multimodal Model\n",
    "\n",
    "Creating a multimodal VQA model with:\n",
    "- **Vision Encoder**: ResNet50 CNN (extracts features from images)\n",
    "- **Text Encoder**: LSTM (processes questions)\n",
    "- **Fusion**: Concatenation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb05a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multimodal model\n",
    "model = create_multimodal_model(\n",
    "    model_type='concat',  # Valid options: 'concat', 'attention', 'bilinear', 'cross_attention'\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=config['text']['embedding_dim'],\n",
    "    text_hidden_dim=config['model']['baseline']['hidden_dim'],\n",
    "    fusion_hidden_dim=config['model']['baseline']['hidden_dim'],\n",
    "    dropout=config['model']['baseline']['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model created successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ebe48",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass\n",
    "\n",
    "Quick sanity check that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_questions = test_batch['question'].to(device)\n",
    "    test_images = test_batch['image'].to(device)\n",
    "    test_answers = test_batch['answer'].to(device)\n",
    "    \n",
    "    outputs = model(test_questions, test_images)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    accuracy = (predictions == test_answers).float().mean()\n",
    "    \n",
    "    print(f\"‚úì Forward pass successful\")\n",
    "    print(f\"  Input questions shape: {test_questions.shape}\")\n",
    "    print(f\"  Input images shape: {test_images.shape}\")\n",
    "    print(f\"  Output shape: {outputs.shape}\")\n",
    "    print(f\"  Random accuracy: {accuracy.item():.4f} (should be ~0.002 for random)\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e2768",
   "metadata": {},
   "source": [
    "## 7. Setup Training\n",
    "\n",
    "Initialize the trainer with early stopping and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = MultimodalVQATrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,  # Pass the full config dictionary\n",
    "    device=device,\n",
    "    checkpoint_dir=project_root / 'checkpoints',\n",
    "    experiment_name='multimodal_concat'\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"  Checkpoint directory: {trainer.checkpoint_dir}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Early stopping patience: 5 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d5666",
   "metadata": {},
   "source": [
    "## 8. Train Model\n",
    "\n",
    "This will take some time (~1-2 hours on CPU, ~15-20 minutes on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8606dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 10 epochs (epochs already configured in config)\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation accuracy: {trainer.best_val_acc:.4f}\")\n",
    "print(f\"Current epoch: {trainer.current_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed1a87",
   "metadata": {},
   "source": [
    "## 9. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since trainer.train() doesn't return history, let's create a summary plot\n",
    "# Based on your training results that showed steady improvement from 28.86% to 50.58%\n",
    "\n",
    "# Create approximate training history from your actual results\n",
    "epochs = list(range(1, 11))\n",
    "train_acc_approx = [0.2798, 0.4039, 0.4634, 0.4891, 0.5111, 0.5277, 0.5456, 0.5603, 0.5705, 0.5778]\n",
    "val_acc_approx = [0.2886, 0.4172, 0.4370, 0.4577, 0.4719, 0.4810, 0.4886, 0.4982, 0.5053, 0.5058]\n",
    "train_loss_approx = [3.8770, 3.1919, 2.9653, 2.8185, 2.6852, 2.5848, 2.5007, 2.4280, 2.3859, 2.3555]\n",
    "val_loss_approx = [3.5777, 3.3643, 3.2973, 3.3337, 3.3527, 3.2975, 3.3170, 3.3624, 3.3535, 3.3633]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(epochs, train_loss_approx, label='Train Loss', marker='o', color='blue')\n",
    "axes[0].plot(epochs, val_loss_approx, label='Val Loss', marker='s', color='orange')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(epochs, train_acc_approx, label='Train Accuracy', marker='o', color='blue')\n",
    "axes[1].plot(epochs, val_acc_approx, label='Val Accuracy', marker='s', color='orange')\n",
    "axes[1].axhline(y=0.4736, color='r', linestyle='--', label='Text-only baseline (47.36%)')\n",
    "axes[1].axhline(y=0.5058, color='g', linestyle=':', alpha=0.7, label='Final multimodal (50.58%)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'multimodal_training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training history plot created\")\n",
    "print(f\"‚úì Plot saved to results/figures/multimodal_training_history.png\")\n",
    "print(f\"üìà Key achievements:\")\n",
    "print(f\"  ‚Ä¢ Started at: 28.86% validation accuracy\")\n",
    "print(f\"  ‚Ä¢ Ended at: 50.58% validation accuracy\") \n",
    "print(f\"  ‚Ä¢ Improvement over text baseline: +3.22 percentage points!\")\n",
    "print(f\"  ‚Ä¢ Total training improvement: +21.72 percentage points from epoch 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ad5e4",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4912b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model checkpoint\n",
    "best_model_path = trainer.checkpoint_dir / 'best_model.pth'\n",
    "if best_model_path.exists():\n",
    "    trainer.load_checkpoint(str(best_model_path))\n",
    "    print(f\"‚úì Loaded best model from: {best_model_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Best model not found at: {best_model_path}\")\n",
    "    print(\"Using current model state for evaluation\")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc, all_preds, all_labels = trainer.evaluate(test_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Text-only baseline: 47.36%\")\n",
    "print(f\"  Multimodal (concat): {test_acc*100:.2f}%\")\n",
    "print(f\"  Improvement: {(test_acc - 0.4736)*100:.2f} percentage points\")\n",
    "print(f\"  Relative improvement: {((test_acc/0.4736 - 1)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f3012",
   "metadata": {},
   "source": [
    "## 11. Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c16e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test samples and predictions\n",
    "model.eval()\n",
    "test_batch = next(iter(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    questions = test_batch['question'].to(device)\n",
    "    images = test_batch['image'].to(device)\n",
    "    true_answers = test_batch['answer'].to(device)\n",
    "    \n",
    "    outputs = model(questions, images)\n",
    "    pred_answers = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(9):\n",
    "    # Denormalize image\n",
    "    img = images[i].cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Get question\n",
    "    question = questions[i].cpu().numpy()\n",
    "    question_text = ' '.join([vocab.idx_to_word.get(idx, '<UNK>') \n",
    "                               for idx in question[:20] if idx > 0])\n",
    "    \n",
    "    # Get answers\n",
    "    true_ans = vocab.idx_to_word.get(true_answers[i].item(), '<UNK>')\n",
    "    pred_ans = vocab.idx_to_word.get(pred_answers[i].item(), '<UNK>')\n",
    "    \n",
    "    # Determine if correct\n",
    "    is_correct = true_answers[i].item() == pred_answers[i].item()\n",
    "    color = 'green' if is_correct else 'red'\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(\n",
    "        f\"Q: {question_text}...\\n\"\n",
    "        f\"True: {true_ans} | Pred: {pred_ans}\",\n",
    "        fontsize=9,\n",
    "        color=color\n",
    "    )\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'multimodal_predictions.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Predictions visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c611d",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb79d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'true_answer': all_labels,\n",
    "    'predicted_answer': all_preds,\n",
    "    'correct': (np.array(all_preds) == np.array(all_labels)).astype(int)\n",
    "})\n",
    "\n",
    "results_path = project_root / 'results' / 'predictions' / 'multimodal_concat_predictions.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"‚úì Results saved to {results_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total predictions: {len(results_df)}\")\n",
    "print(f\"  Correct: {results_df['correct'].sum()}\")\n",
    "print(f\"  Incorrect: {len(results_df) - results_df['correct'].sum()}\")\n",
    "print(f\"  Accuracy: {results_df['correct'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405340e2",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "üéØ **Achieved Goals:**\n",
    "- ‚úÖ Implemented multimodal VQA with vision + text\n",
    "- ‚úÖ Trained concatenation fusion model\n",
    "- ‚úÖ Compared with text-only baseline (47.36%)\n",
    "\n",
    "üìà **Potential Improvements:**\n",
    "\n",
    "1. **Try different fusion strategies:**\n",
    "   - Attention fusion (better weighting)\n",
    "   - Bilinear fusion (richer interactions)\n",
    "   - Cross-modal attention (full co-attention)\n",
    "\n",
    "2. **Better vision encoder:**\n",
    "   - Use AttentionVisionEncoder with spatial attention\n",
    "   - Try different CNN backbones (ResNet101, EfficientNet)\n",
    "   - Fine-tune vision encoder instead of freezing\n",
    "\n",
    "3. **Data augmentation:**\n",
    "   - Stronger image augmentation\n",
    "   - Text augmentation (paraphrasing)\n",
    "\n",
    "4. **Hyperparameter tuning:**\n",
    "   - Learning rate scheduling\n",
    "   - Different batch sizes\n",
    "   - Gradient clipping\n",
    "\n",
    "5. **Pre-trained models:**\n",
    "   - Use CLIP or ViLT\n",
    "   - Fine-tune BERT for questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
