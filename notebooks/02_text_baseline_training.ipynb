{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbfaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup (auto-detects environment)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install packages\n",
    "    print(\"Installing packages...\")\n",
    "    !pip install -q torch torchvision tqdm pyyaml scikit-learn pandas matplotlib seaborn\n",
    "    \n",
    "    # Set project path\n",
    "    import os\n",
    "    PROJECT_PATH = \"/content/drive/MyDrive/WOA7015 Advanced Machine Learning/data\"\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"✓ Running on Colab - Path: {PROJECT_PATH}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Running locally\n",
    "    PROJECT_PATH = None\n",
    "    print(\"✓ Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f1db2",
   "metadata": {},
   "source": [
    "## 0. Setup (Run this first)\n",
    "\n",
    "**For Google Colab**: This cell will automatically mount your Drive and install packages.\n",
    "**For Local**: This cell will skip Colab-specific setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232d814",
   "metadata": {},
   "source": [
    "# Text-Only VQA Baseline Training\n",
    "\n",
    "**Goal**: Train a language model to answer medical questions based on text only (without images)\n",
    "\n",
    "This notebook walks through:\n",
    "1. Loading and exploring the data\n",
    "2. Building vocabulary from questions\n",
    "3. Creating an LSTM model\n",
    "4. Training the model\n",
    "5. Evaluating performance\n",
    "6. Analyzing results\n",
    "\n",
    "**Note**: Can run on Google Colab (free tier) - no GPU needed for text-only baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0260ba",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload src modules (run this if you see import errors after updating code)\n",
    "import sys\n",
    "modules_to_remove = [m for m in sys.modules if m.startswith('src')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "print(f\"✓ Cleared {len(modules_to_remove)} cached modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336100c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "if 'PROJECT_PATH' in globals() and PROJECT_PATH:\n",
    "    # Colab environment\n",
    "    project_root = Path(PROJECT_PATH)\n",
    "else:\n",
    "    # Local environment\n",
    "    project_root = Path().absolute().parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataset import TextOnlyVQADataset, create_text_dataloaders\n",
    "from src.models.text_model import LSTMTextModel, create_text_model\n",
    "from src.training.trainer import TextVQATrainer\n",
    "from src.evaluation.metrics import VQAMetrics, calculate_accuracy\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6afadd",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightweight configuration (or create default if not exists)\n",
    "config_path = project_root / \"config_lightweight.yaml\"\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"✓ Loaded config_lightweight.yaml\")\n",
    "else:\n",
    "    # Create default lightweight config for Colab\n",
    "    print(\"⚠️  config_lightweight.yaml not found, using default settings\")\n",
    "    config = {\n",
    "        'data': {'train_csv': 'trainrenamed.csv', 'test_csv': 'testrenamed.csv', \n",
    "                 'answers_file': 'answers.txt', 'val_split': 0.15},\n",
    "        'text': {'max_length': 32, 'embedding_dim': 128},\n",
    "        'model': {'baseline': {'hidden_dim': 256, 'num_layers': 1, 'dropout': 0.2}},\n",
    "        'training': {'batch_size': 8, 'num_epochs': 10, 'learning_rate': 0.001, \n",
    "                    'weight_decay': 0.0001, 'scheduler': 'step', 'gradient_clip': 1.0,\n",
    "                    'early_stopping_patience': 5},\n",
    "        'paths': {'checkpoints': 'checkpoints'},\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Hidden dim: {config['model']['baseline']['hidden_dim']}\")\n",
    "print(f\"  Embedding dim: {config['text']['embedding_dim']}\")\n",
    "print(f\"  Max sequence length: {config['text']['max_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe83db9",
   "metadata": {},
   "source": [
    "## 3. Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d841f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv(project_root / config['data']['train_csv'])\n",
    "test_df = pd.read_csv(project_root / config['data']['test_csv'])\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "print(f\"\\nFirst few samples:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze questions\n",
    "train_df['question_length'] = train_df['question'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Question length distribution\n",
    "axes[0].hist(train_df['question_length'], bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Question Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Question Lengths')\n",
    "axes[0].axvline(train_df['question_length'].mean(), color='red', linestyle='--', label=f'Mean: {train_df[\"question_length\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Answer distribution (top 20)\n",
    "top_answers = train_df['answer'].value_counts().head(20)\n",
    "axes[1].barh(range(len(top_answers)), top_answers.values)\n",
    "axes[1].set_yticks(range(len(top_answers)))\n",
    "axes[1].set_yticklabels(top_answers.index, fontsize=8)\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_title('Top 20 Most Common Answers')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Question length statistics:\")\n",
    "print(f\"  Mean: {train_df['question_length'].mean():.2f} words\")\n",
    "print(f\"  Median: {train_df['question_length'].median():.0f} words\")\n",
    "print(f\"  Max: {train_df['question_length'].max():.0f} words\")\n",
    "print(f\"\\nUnique answers: {train_df['answer'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze question types\n",
    "def classify_question_type(answer):\n",
    "    answer_lower = str(answer).lower().strip()\n",
    "    if answer_lower in ['yes', 'no']:\n",
    "        return 'yes/no'\n",
    "    else:\n",
    "        return 'open-ended'\n",
    "\n",
    "train_df['question_type'] = train_df['answer'].apply(classify_question_type)\n",
    "\n",
    "question_type_counts = train_df['question_type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(question_type_counts.values, labels=question_type_counts.index, \n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Question Types Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(\"Question type breakdown:\")\n",
    "for qtype, count in question_type_counts.items():\n",
    "    print(f\"  {qtype}: {count:,} ({count/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b33699",
   "metadata": {},
   "source": [
    "## 4. Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "\n",
    "train_loader, val_loader, test_loader, vocab_size, num_classes, vocab = create_text_dataloaders(\n",
    "    train_csv=str(project_root / config['data']['train_csv']),\n",
    "    test_csv=str(project_root / config['data']['test_csv']),\n",
    "    answers_file=str(project_root / config['data']['answers_file']),\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    val_split=config['data']['val_split'],\n",
    "    num_workers=0,\n",
    "    max_length=config['text']['max_length']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataloaders created:\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  Number of answer classes: {num_classes:,}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c1cda0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Inspect a sample batch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sample_batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample batch:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Question tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_batch[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Sample batch:\")\n",
    "print(f\"  Question tensor shape: {sample_batch['question'].shape}\")\n",
    "print(f\"  Answer tensor shape: {sample_batch['answer'].shape}\")\n",
    "print(f\"\\nFirst question (encoded): {sample_batch['question'][0][:20].tolist()}...\")\n",
    "print(f\"First question (text): {sample_batch['question_text'][0]}\")\n",
    "print(f\"First answer: {sample_batch['answer_text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vocabulary statistics\n",
    "print(\"Vocabulary sample (first 20 tokens):\")\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])[:20]\n",
    "for token, idx in sorted_vocab:\n",
    "    print(f\"  {idx:3d}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a74cd7",
   "metadata": {},
   "source": [
    "## 5. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = create_text_model(\n",
    "    model_type='lstm',\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=config['text']['embedding_dim'],\n",
    "    hidden_dim=config['model']['baseline']['hidden_dim'],\n",
    "    num_layers=config['model']['baseline']['num_layers'],\n",
    "    dropout=config['model']['baseline']['dropout'],\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ Model created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.2f} MB\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_questions = sample_batch['question'][:4].to(device)\n",
    "    test_outputs = model(test_questions)\n",
    "    test_probs = torch.softmax(test_outputs, dim=1)\n",
    "    test_preds = test_outputs.argmax(dim=1)\n",
    "\n",
    "print(\"Test forward pass:\")\n",
    "print(f\"  Input shape: {test_questions.shape}\")\n",
    "print(f\"  Output shape: {test_outputs.shape}\")\n",
    "print(f\"  Predictions: {test_preds.cpu().numpy()}\")\n",
    "print(f\"  Max probabilities: {test_probs.max(dim=1)[0].cpu().numpy()}\")\n",
    "print(\"\\n✓ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4030d9c",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692260cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(config['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config['seed'])\n",
    "\n",
    "# Create trainer\n",
    "trainer = TextVQATrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    checkpoint_dir=str(project_root / config['paths']['checkpoints']),\n",
    "    experiment_name=\"text_baseline_lstm_notebook\"\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"  Checkpoints will be saved to: {trainer.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70405d07",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "**Note**: This will take some time depending on your laptop specs:\n",
    "- **Quick test (2 epochs)**: ~5-10 minutes\n",
    "- **Full training (10 epochs)**: ~30-60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Reduce epochs for quick testing\n",
    "# config['training']['num_epochs'] = 2  # Uncomment for quick test\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"Best validation accuracy: {trainer.best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd64c6",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = trainer.checkpoint_dir / \"best_model.pth\"\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"✓ Loaded best model\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Best val accuracy: {checkpoint['best_val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_metrics = VQAMetrics(num_classes)\n",
    "\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        questions = batch['question'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        \n",
    "        outputs = model(questions)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        \n",
    "        test_metrics.update(\n",
    "            predictions,\n",
    "            answers,\n",
    "            batch.get('question_text'),\n",
    "            batch.get('answer_text')\n",
    "        )\n",
    "\n",
    "# Compute metrics\n",
    "metrics = test_metrics.compute()\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score (macro):   {metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1 Score (weighted):{metrics['f1_weighted']:.4f}\")\n",
    "print(f\"Precision (macro):  {metrics['precision_macro']:.4f}\")\n",
    "print(f\"Recall (macro):     {metrics['recall_macro']:.4f}\")\n",
    "print(f\"Exact Match:        {metrics['exact_match']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb718e",
   "metadata": {},
   "source": [
    "## 9. Analyze Results by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-question-type analysis\n",
    "per_type_metrics = test_metrics.compute_per_question_type()\n",
    "\n",
    "if per_type_metrics:\n",
    "    print(\"\\nPerformance by Question Type:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for qtype, stats in per_type_metrics.items():\n",
    "        print(f\"\\n{qtype.upper()}:\")\n",
    "        print(f\"  Count: {stats['count']:,}\")\n",
    "        print(f\"  Accuracy: {stats['accuracy']:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    types = list(per_type_metrics.keys())\n",
    "    accs = [per_type_metrics[t]['accuracy'] for t in types]\n",
    "    \n",
    "    bars = ax.bar(types, accs, color=['skyblue', 'coral'])\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy by Question Type')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479acd3f",
   "metadata": {},
   "source": [
    "## 10. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion statistics\n",
    "confusion_stats = test_metrics.get_confusion_stats(top_k=10)\n",
    "\n",
    "print(\"\\nTop 10 Most Confused Answer Pairs:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'True Answer':<25} {'Predicted Answer':<25} {'Count':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get answer vocabulary\n",
    "with open(project_root / config['data']['answers_file'], 'r', encoding='utf-8') as f:\n",
    "    answers = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for (true_idx, pred_idx), count in confusion_stats['top_confusions']:\n",
    "    true_ans = answers[true_idx] if true_idx < len(answers) else f\"idx_{true_idx}\"\n",
    "    pred_ans = answers[pred_idx] if pred_idx < len(answers) else f\"idx_{pred_idx}\"\n",
    "    print(f\"{true_ans:<25} {pred_ans:<25} {count:>10}\")\n",
    "\n",
    "print(f\"\\nTotal errors: {confusion_stats['total_errors']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6abee",
   "metadata": {},
   "source": [
    "## 11. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "model.eval()\n",
    "sample_batch = next(iter(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    questions = sample_batch['question'].to(device)\n",
    "    outputs = model(questions)\n",
    "    predictions, probabilities = model.predict(questions)\n",
    "\n",
    "# Show first 10 examples\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i in range(min(10, len(sample_batch['question_text']))):\n",
    "    question = sample_batch['question_text'][i]\n",
    "    true_answer = sample_batch['answer_text'][i]\n",
    "    pred_idx = predictions[i].item()\n",
    "    pred_answer = answers[pred_idx] if pred_idx < len(answers) else f\"idx_{pred_idx}\"\n",
    "    confidence = probabilities[i].max().item()\n",
    "    \n",
    "    is_correct = \"✓\" if pred_answer.lower() == true_answer.lower() else \"✗\"\n",
    "    \n",
    "    print(f\"\\n{is_correct} Example {i+1}:\")\n",
    "    print(f\"  Question: {question}\")\n",
    "    print(f\"  True Answer: {true_answer}\")\n",
    "    print(f\"  Predicted: {pred_answer} (confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea717d58",
   "metadata": {},
   "source": [
    "## 12. Save Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "results_summary = {\n",
    "    'model_type': 'LSTM',\n",
    "    'vocab_size': vocab_size,\n",
    "    'num_classes': num_classes,\n",
    "    'total_parameters': total_params,\n",
    "    'test_accuracy': metrics['accuracy'],\n",
    "    'test_f1_macro': metrics['f1_macro'],\n",
    "    'test_f1_weighted': metrics['f1_weighted'],\n",
    "    'test_precision': metrics['precision_macro'],\n",
    "    'test_recall': metrics['recall_macro'],\n",
    "    'best_val_accuracy': trainer.best_val_acc,\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "results_dir = project_root / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import json\n",
    "with open(results_dir / 'text_baseline_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"✓ Results saved to results/text_baseline_results.json\")\n",
    "print(\"\\nSummary:\")\n",
    "for key, value in results_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf87320",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Next Steps\n",
    "\n",
    "### What we've achieved:\n",
    "- ✅ Built vocabulary from medical questions\n",
    "- ✅ Trained LSTM model to predict answers from text only\n",
    "- ✅ Evaluated performance on test set\n",
    "- ✅ Analyzed results by question type\n",
    "\n",
    "### Expected Performance:\n",
    "- **Text-only baseline**: 15-30% accuracy\n",
    "- This is significantly better than random (0.02% for 4,593 classes)\n",
    "- But limited without visual information!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Add Vision**: Implement CNN image encoder\n",
    "2. **Multimodal Fusion**: Combine text + image features\n",
    "3. **Attention Mechanisms**: Let model focus on relevant image regions\n",
    "4. **Pre-trained VLMs**: Use BLIP, CLIP, or similar models\n",
    "5. **Fine-tuning**: Optimize on PathVQA dataset\n",
    "\n",
    "### Model Checkpoint:\n",
    "Your trained model is saved at:\n",
    "```\n",
    "checkpoints/text_baseline_lstm_notebook/best_model.pth\n",
    "```\n",
    "\n",
    "You can load it later for inference or further training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
