{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import existing modules\n",
    "from src.data.dataset import create_multimodal_dataloaders\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed418a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced vocabulary with top 1000 classes\n",
    "def create_reduced_vocabulary(train_csv_path, top_n=1000):\n",
    "    \"\"\"Create vocabulary with only top N most frequent answers\"\"\"\n",
    "    \n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # Count answer frequencies\n",
    "    answer_counts = Counter(train_df['answer'])\n",
    "    \n",
    "    # Get top N answers\n",
    "    top_answers = [answer for answer, count in answer_counts.most_common(top_n)]\n",
    "    \n",
    "    # Create mapping\n",
    "    answer_to_idx = {answer: idx for idx, answer in enumerate(top_answers)}\n",
    "    answer_to_idx['<UNK>'] = len(answer_to_idx)\n",
    "    \n",
    "    # Save reduced answers file\n",
    "    dataset_path = project_root / 'data'\n",
    "    reduced_answers_path = dataset_path / f'answers_top_{top_n}.txt'\n",
    "    \n",
    "    with open(reduced_answers_path, 'w') as f:\n",
    "        for answer in top_answers:\n",
    "            f.write(f\"{answer}\\n\")\n",
    "        f.write(\"<UNK>\\n\")\n",
    "    \n",
    "    coverage = sum(answer_counts[ans] for ans in top_answers) / sum(answer_counts.values()) * 100\n",
    "    \n",
    "    print(f\"Created reduced vocabulary:\")\n",
    "    print(f\"  Classes: {len(answer_to_idx)}\")\n",
    "    print(f\"  Coverage: {coverage:.1f}% of training data\")\n",
    "    print(f\"  Saved to: {reduced_answers_path}\")\n",
    "    \n",
    "    return str(reduced_answers_path), answer_to_idx\n",
    "\n",
    "# Create reduced vocabulary\n",
    "dataset_path = project_root / 'data'\n",
    "reduced_answers_file, reduced_answer_to_idx = create_reduced_vocabulary(\n",
    "    train_csv_path=str(dataset_path / 'trainrenamed.csv'),\n",
    "    top_n=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data transforms with augmentation\n",
    "def get_enhanced_transforms():\n",
    "    \"\"\"Enhanced data augmentation\"\"\"\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.08))\n",
    "    ])\n",
    "    \n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "train_transforms, val_transforms = get_enhanced_transforms()\n",
    "print(\"Enhanced data transforms created with augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4fbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with reduced vocabulary\n",
    "train_loader, val_loader, test_loader, vocab_size, num_classes, vocab, answer_to_idx = create_multimodal_dataloaders(\n",
    "    train_csv=str(dataset_path / 'trainrenamed.csv'),\n",
    "    test_csv=str(dataset_path / 'testrenamed.csv'),\n",
    "    image_dir=str(dataset_path / 'train'),\n",
    "    answers_file=reduced_answers_file,  # Use reduced vocabulary\n",
    "    batch_size=32,  # Increased batch size\n",
    "    val_split=0.1,\n",
    "    num_workers=2,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "print(f\"Data loaded with improvements:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size increased to: 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1392d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved multimodal model\n",
    "class ImprovedMultimodalVQA(nn.Module):\n",
    "    \"\"\"Enhanced multimodal VQA with optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim=300, \n",
    "                 text_hidden_dim=512, fusion_hidden_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.text_lstm = nn.LSTM(embedding_dim, text_hidden_dim, \n",
    "                                batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.text_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Vision encoder (trainable)\n",
    "        self.vision_encoder = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.vision_encoder = nn.Sequential(*list(self.vision_encoder.children())[:-2])\n",
    "        \n",
    "        # Simplified spatial attention\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2048, 256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Cross-modal fusion\n",
    "        self.vision_proj = nn.Linear(2048, fusion_hidden_dim)\n",
    "        self.text_proj = nn.Linear(text_hidden_dim * 2, fusion_hidden_dim)\n",
    "        \n",
    "        # Reduced attention heads for efficiency\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=fusion_hidden_dim, \n",
    "            num_heads=4,  # Reduced from 8\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Enhanced classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_hidden_dim, fusion_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_hidden_dim, fusion_hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, questions, images):\n",
    "        # Text processing\n",
    "        text_embedded = self.text_embedding(questions)\n",
    "        text_output, (text_hidden, _) = self.text_lstm(text_embedded)\n",
    "        text_features = torch.cat([text_hidden[0], text_hidden[1]], dim=1)\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        \n",
    "        # Vision processing with spatial attention\n",
    "        vision_maps = self.vision_encoder(images)\n",
    "        attention_weights = self.spatial_attention(vision_maps)\n",
    "        attended_vision = vision_maps * attention_weights\n",
    "        vision_features = nn.functional.adaptive_avg_pool2d(attended_vision, 1).squeeze()\n",
    "        \n",
    "        # Handle batch dimension\n",
    "        if len(vision_features.shape) == 1:\n",
    "            vision_features = vision_features.unsqueeze(0)\n",
    "        \n",
    "        # Project and fuse\n",
    "        vision_proj = self.vision_proj(vision_features).unsqueeze(1)\n",
    "        text_proj = self.text_proj(text_features).unsqueeze(1)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        attended_features, _ = self.cross_attention(\n",
    "            query=text_proj.transpose(0, 1),\n",
    "            key=vision_proj.transpose(0, 1),\n",
    "            value=vision_proj.transpose(0, 1)\n",
    "        )\n",
    "        \n",
    "        fused_features = attended_features.transpose(0, 1).squeeze(1)\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create improved model\n",
    "model = ImprovedMultimodalVQA(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=300,\n",
    "    text_hidden_dim=512,\n",
    "    fusion_hidden_dim=512,\n",
    "    dropout=0.1  # Reduced dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"Improved model created:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Reduced dropout: 0.1\")\n",
    "print(f\"  Simplified attention: 4 heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036058c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training setup\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1, gamma=2, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(\n",
    "            inputs, targets, \n",
    "            reduction='none',\n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Setup enhanced training\n",
    "vision_params = []\n",
    "other_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'vision_encoder' in name:\n",
    "        vision_params.append(param)\n",
    "    else:\n",
    "        other_params.append(param)\n",
    "\n",
    "# Enhanced optimizer with higher learning rates\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': vision_params, 'lr': 1e-5, 'weight_decay': 1e-4},    # Higher for vision\n",
    "    {'params': other_params, 'lr': 1e-4, 'weight_decay': 1e-4}     # Higher for new layers  \n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# More stable scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 15], gamma=0.3)\n",
    "\n",
    "# Focal loss with label smoothing\n",
    "criterion = FocalLoss(alpha=1, gamma=2, label_smoothing=0.1)\n",
    "\n",
    "print(\"Enhanced training setup:\")\n",
    "print(f\"  Vision parameters: {len(vision_params):,}\")\n",
    "print(f\"  Other parameters: {len(other_params):,}\")\n",
    "print(f\"  Vision LR: 1e-5\")\n",
    "print(f\"  Other LR: 1e-4\") \n",
    "print(f\"  Using Focal Loss with label smoothing\")\n",
    "print(f\"  MultiStepLR scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a58ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training loop\n",
    "def train_epoch_improved(model, train_loader, val_loader, optimizer, criterion, scheduler, device, epoch, total_epochs):\n",
    "    \"\"\"Enhanced training loop\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{total_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Handle dictionary format\n",
    "        if isinstance(batch, dict):\n",
    "            questions = batch['question'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "        else:\n",
    "            questions, images, answers = batch\n",
    "            questions, images, answers = questions.to(device), images.to(device), answers.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(questions, images)\n",
    "        loss = criterion(outputs, answers)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(loss.item())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += answers.size(0)\n",
    "        train_correct += (predicted == answers).sum().item()\n",
    "        \n",
    "        if batch_idx % 30 == 0:  # More frequent updates\n",
    "            current_acc = 100. * train_correct / train_total\n",
    "            print(f\"Batch {batch_idx:3d}/{len(train_loader):3d} | Loss: {loss.item():.4f} | Acc: {current_acc:.2f}%\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    train_accuracy = 100. * train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if isinstance(batch, dict):\n",
    "                questions = batch['question'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "            else:\n",
    "                questions, images, answers = batch\n",
    "                questions, images, answers = questions.to(device), images.to(device), answers.to(device)\n",
    "            \n",
    "            outputs = model(questions, images)\n",
    "            loss = criterion(outputs, answers)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += answers.size(0)\n",
    "            val_correct += (predicted == answers).sum().item()\n",
    "    \n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_accuracy = 100. * val_correct / val_total\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_accuracy:.2f}%\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {\n",
    "        'train_loss': avg_train_loss,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    }\n",
    "\n",
    "print(\"Enhanced training function ready\")\n",
    "print(\"This should show significant improvement over the current 25% plateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56eaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run improved training\n",
    "EPOCHS = 20\n",
    "history = {\n",
    "    'train_losses': [],\n",
    "    'train_accuracies': [],\n",
    "    'val_losses': [],\n",
    "    'val_accuracies': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting improved multimodal training...\")\n",
    "print(f\"Training for {EPOCHS} epochs with all optimizations\")\n",
    "print(f\"Expected target: 35-45% accuracy (vs current 25%)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_results = train_epoch_improved(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer, criterion, scheduler, \n",
    "            device, epoch, EPOCHS\n",
    "        )\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(epoch_results['train_loss'])\n",
    "        history['train_accuracies'].append(epoch_results['train_accuracy'])\n",
    "        history['val_losses'].append(epoch_results['val_loss'])\n",
    "        history['val_accuracies'].append(epoch_results['val_accuracy'])\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_results['val_accuracy'] > best_val_acc:\n",
    "            best_val_acc = epoch_results['val_accuracy']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = project_root / 'checkpoints' / 'improved_multimodal' / 'best_model.pth'\n",
    "            checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history,\n",
    "                'num_classes': num_classes\n",
    "            }, checkpoint_path)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time/60:.2f} minutes\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize improved training results\n",
    "if len(history['train_losses']) > 0:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs_range = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs_range, history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs_range, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves  \n",
    "    ax2.plot(epochs_range, history['train_accuracies'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs_range, history['val_accuracies'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.axhline(y=47.36, color='g', linestyle='--', label='Text Baseline (47.36%)', alpha=0.7)\n",
    "    ax2.axhline(y=25, color='orange', linestyle=':', label='Previous Best (25%)', alpha=0.7)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Improvement tracking\n",
    "    max_val_acc = max(history['val_accuracies'])\n",
    "    improvement_from_previous = max_val_acc - 25.0\n",
    "    \n",
    "    ax3.bar(['Previous Best', 'Improved Model'], [25.0, max_val_acc], \n",
    "           color=['orange', 'green'], alpha=0.7)\n",
    "    ax3.axhline(y=47.36, color='red', linestyle='--', alpha=0.7, label='Target (47.36%)')\n",
    "    ax3.set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Summary statistics\n",
    "    ax4.axis('off')\n",
    "    stats_text = f\"\"\"Improved Training Results:\n",
    "    \n",
    "Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%\n",
    "Best Validation Accuracy: {max_val_acc:.2f}%\n",
    "Improvement over Previous: +{improvement_from_previous:.2f}%\n",
    "Target Achievement: {max_val_acc/47.36*100:.1f}%\n",
    "    \n",
    "Total Training Epochs: {len(history['train_losses'])}\n",
    "Final Training Loss: {history['train_losses'][-1]:.4f}\n",
    "Final Validation Loss: {history['val_losses'][-1]:.4f}\n",
    "    \n",
    "Key Improvements Applied:\n",
    "- Reduced classes: 4,142 → 1,000\n",
    "- Enhanced data augmentation\n",
    "- Higher learning rates\n",
    "- Focal loss with label smoothing\n",
    "- Optimized architecture\"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    ax4.set_title('Training Summary', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key results\n",
    "    print(\"Improved Multimodal Training Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Best Validation Accuracy: {max_val_acc:.2f}%\")\n",
    "    print(f\"Improvement: +{improvement_from_previous:.2f}% over previous\")\n",
    "    print(f\"Target Progress: {max_val_acc/47.36*100:.1f}% toward 47.36% baseline\")\n",
    "    \n",
    "    if max_val_acc > 47.36:\n",
    "        print(\"SUCCESS: Exceeded text baseline!\")\n",
    "    elif max_val_acc > 35:\n",
    "        print(\"GOOD: Significant improvement achieved\")\n",
    "    else:\n",
    "        print(\"PROGRESS: Some improvement, may need further tuning\")\n",
    "        \n",
    "else:\n",
    "    print(\"No training history available - run training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20038e7c",
   "metadata": {},
   "source": [
    "## Key Improvements Summary\n",
    "\n",
    "This improved training setup addresses the main bottlenecks:\n",
    "\n",
    "1. **Class Reduction**: 4,142 → 1,000 classes (biggest impact)\n",
    "2. **Enhanced Data Augmentation**: Better generalization \n",
    "3. **Higher Learning Rates**: Faster convergence\n",
    "4. **Focal Loss**: Handles class imbalance\n",
    "5. **Optimized Architecture**: 4 attention heads, reduced dropout\n",
    "6. **Better Scheduling**: More stable MultiStepLR\n",
    "\n",
    "**Expected Results**: 35-45% accuracy vs previous 25% plateau\n",
    "**Target**: Beat 47.36% text baseline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
