{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76dff098",
      "metadata": {
        "id": "76dff098"
      },
      "source": [
        "# Multimodal VQA Fine-tuning Notebook\n",
        "\n",
        "This notebook provides comprehensive fine-tuning strategies for improving multimodal Visual Question Answering performance.\n",
        "\n",
        "**Environment Support:**\n",
        "- Local VS Code/Jupyter\n",
        "- Google Colab  \n",
        "- Other Jupyter environments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1baa0a21",
      "metadata": {
        "id": "1baa0a21"
      },
      "source": [
        "## 0. Environment Setup & Path Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5feea046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5feea046",
        "outputId": "4160e651-22f6-4f78-88cf-a452c0dc2ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment detected: Google Colab\n",
            "Setting up Google Colab environment...\n",
            "Mounted at /content/drive\n",
            "Installing packages...\n",
            "SUCCESS: Colab project root: /content/drive/MyDrive/Colab Notebooks/WOA7015 Advanced Machine Learning\n",
            "Found: src/\n",
            "Found: data/\n",
            "Found: notebooks/\n",
            "Found: checkpoints/\n",
            "\n",
            "Environment ready!\n",
            "   Project root: /content/drive/MyDrive/Colab Notebooks/WOA7015 Advanced Machine Learning\n",
            "   Running on Colab: True\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# UNIVERSAL ENVIRONMENT SETUP - Works on Colab, VS Code, and Jupyter\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Automatically detect environment and setup project paths.\n",
        "    Supports: Google Colab, VS Code, Local Jupyter\n",
        "    \"\"\"\n",
        "\n",
        "    # Detect if running on Google Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Environment detected: Google Colab\")\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"Environment detected: Local (VS Code/Jupyter)\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        # ========== GOOGLE COLAB SETUP ==========\n",
        "        print(\"Setting up Google Colab environment...\")\n",
        "\n",
        "        # Mount Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Install required packages\n",
        "        print(\"Installing packages...\")\n",
        "        os.system('pip install -q torch torchvision tqdm pyyaml scikit-learn pandas matplotlib seaborn Pillow')\n",
        "\n",
        "        # Set project path (adjust this path to your Google Drive structure)\n",
        "        #\"/content/drive/MyDrive/Colab Notebooks/WOA7015 Advanced Machine Learning\"\n",
        "        PROJECT_PATH = \"/content/drive/MyDrive/Colab Notebooks/WOA7015 Advanced Machine Learning\"\n",
        "\n",
        "        # Alternative paths - uncomment the one that matches your Drive structure\n",
        "        # PROJECT_PATH = \"/content/drive/MyDrive/data\"\n",
        "        # PROJECT_PATH = \"/content/drive/MyDrive/WOA7015 Advanced Machine Learning/data\"\n",
        "\n",
        "        if os.path.exists(PROJECT_PATH):\n",
        "            os.chdir(PROJECT_PATH)\n",
        "            project_root = Path(PROJECT_PATH)\n",
        "            print(f\"SUCCESS: Colab project root: {PROJECT_PATH}\")\n",
        "        else:\n",
        "            print(f\"ERROR: Project path not found: {PROJECT_PATH}\")\n",
        "            print(\"Available paths in Drive:\")\n",
        "            base_path = \"/content/drive/MyDrive\"\n",
        "            if os.path.exists(base_path):\n",
        "                for item in os.listdir(base_path):\n",
        "                    print(f\"   - {os.path.join(base_path, item)}\")\n",
        "            raise FileNotFoundError(f\"Please update PROJECT_PATH in the code to match your Google Drive structure\")\n",
        "\n",
        "    else:\n",
        "        # ========== LOCAL ENVIRONMENT SETUP ==========\n",
        "        print(\"Setting up local environment...\")\n",
        "\n",
        "        # Determine project root (works from notebooks/ subdirectory or project root)\n",
        "        current_dir = Path().absolute()\n",
        "\n",
        "        if current_dir.name == 'notebooks':\n",
        "            project_root = current_dir.parent\n",
        "            print(\"Running from notebooks/ directory\")\n",
        "        else:\n",
        "            project_root = current_dir\n",
        "            print(\"Running from project root directory\")\n",
        "\n",
        "        print(f\"SUCCESS: Local project root: {project_root}\")\n",
        "\n",
        "    # ========== COMMON SETUP FOR ALL ENVIRONMENTS ==========\n",
        "\n",
        "    # Add paths to Python path\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    sys.path.insert(0, str(project_root / 'src'))\n",
        "\n",
        "    # Verify project structure\n",
        "    required_dirs = ['src', 'data', 'notebooks', 'checkpoints']\n",
        "    missing_dirs = []\n",
        "\n",
        "    for req_dir in required_dirs:\n",
        "        dir_path = project_root / req_dir\n",
        "        if dir_path.exists():\n",
        "            print(f\"Found: {req_dir}/\")\n",
        "        else:\n",
        "            missing_dirs.append(req_dir)\n",
        "            print(f\"Missing: {req_dir}/\")\n",
        "\n",
        "    if missing_dirs:\n",
        "        print(f\"\\nWarning: Some directories are missing: {missing_dirs}\")\n",
        "        print(\"   Make sure you're running from the correct project directory.\")\n",
        "\n",
        "    return project_root, IN_COLAB\n",
        "\n",
        "# Run environment setup\n",
        "project_root, is_colab = setup_environment()\n",
        "\n",
        "print(f\"\\nEnvironment ready!\")\n",
        "print(f\"   Project root: {project_root}\")\n",
        "print(f\"   Running on Colab: {is_colab}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3db32a",
      "metadata": {
        "id": "4c3db32a"
      },
      "source": [
        "### Copy this setup to other notebooks:\n",
        "\n",
        "**For consistent environment setup across all notebooks, copy the cell above to:**\n",
        "- `01_data_exploration.ipynb`  \n",
        "- `02_text_baseline_training.ipynb`\n",
        "- `03_multimodal_training.ipynb`\n",
        "- `improved_multimodal_training.ipynb`\n",
        "- Any new notebooks\n",
        "\n",
        "**Customization for Colab:**\n",
        "- Update the `PROJECT_PATH` variable in the setup cell to match your Google Drive folder structure\n",
        "- Common paths:\n",
        "  - `/content/drive/MyDrive/WOA7015 Advanced Machine Learning/my_projct`\n",
        "  - `/content/drive/MyDrive/data`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0846e75",
      "metadata": {
        "id": "a0846e75"
      },
      "source": [
        "### üöÄ Alternative: Simple One-Line Setup\n",
        "\n",
        "If you prefer a simpler approach, you can use the standalone setup module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "940f41e4",
      "metadata": {
        "id": "940f41e4"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üöÄ ALTERNATIVE: Simple One-Line Setup (using setup_environment.py)\n",
        "# ============================================================================\n",
        "# Uncomment and run this instead of the detailed setup above\n",
        "\n",
        "# from setup_environment import quick_setup\n",
        "# project_root, device, is_colab, modules = quick_setup()\n",
        "\n",
        "# # Access imported modules\n",
        "# create_multimodal_dataloaders = modules['create_multimodal_dataloaders']\n",
        "# ImprovedMultimodalVQA = modules['ImprovedMultimodalVQA']\n",
        "\n",
        "# # Import other required packages\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torchvision import transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from tqdm import tqdm\n",
        "# import json, time, numpy as np\n",
        "\n",
        "# print(\"‚úÖ One-line setup complete!\")\n",
        "\n",
        "# ============================================================================\n",
        "# For Google Colab with custom path:\n",
        "# project_root, device, is_colab, modules = quick_setup(\n",
        "#     colab_project_path=\"/content/drive/MyDrive/your-custom-path\"\n",
        "# )\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae04436",
      "metadata": {
        "id": "bae04436"
      },
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c8ff9ce0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8ff9ce0",
        "outputId": "ffa3891c-ed2e-4f6a-b4ed-c423d77b833b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: Project modules imported successfully\n",
            "GPU available: True\n",
            "Using device: cuda\n",
            "   GPU: Tesla T4\n",
            "   Memory: 14.7 GB\n",
            "Project root: /content/drive/MyDrive/Colab Notebooks/WOA7015 Advanced Machine Learning\n",
            "Setup complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries (using standardized project_root from above)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import yaml\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import project modules (project_root already set in cell above)\n",
        "try:\n",
        "    from src.data.dataset import create_multimodal_dataloaders\n",
        "    from src.models.improved_multimodal_model import ImprovedMultimodalVQA\n",
        "    print(\"SUCCESS: Project modules imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: Import error: {e}\")\n",
        "    print(\"   Make sure the environment setup cell above ran successfully\")\n",
        "    print(\"   and the project structure is correct\")\n",
        "    raise\n",
        "\n",
        "# Device setup\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(\"Setup complete!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef29d69",
      "metadata": {
        "id": "2ef29d69"
      },
      "source": [
        "## 2. Fine-tuning Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03249a03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03249a03",
        "outputId": "9447f97e-7bc8-4223-de93-333fedcb9b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Strategy: Conservative Fine-tuning\n",
            "Description: Lower learning rates, maintain model stability\n",
            "Epochs: 5, Learning Rate: 1e-06\n"
          ]
        }
      ],
      "source": [
        "# Fine-tuning strategy selector\n",
        "FINE_TUNING_STRATEGY = \"layerwise\"  # Changed from \"conservative\" - try layerwise with better learning rates\n",
        "\n",
        "# Enhanced configuration with better hyperparameters\n",
        "FINE_TUNING_CONFIG = {\n",
        "    \"conservative\": {\n",
        "        \"name\": \"Conservative Fine-tuning\",\n",
        "        \"description\": \"Lower learning rates, maintain model stability\",\n",
        "        \"learning_rate\": 5e-6,  # Increased from 1e-6 - was too low\n",
        "        \"vision_lr_factor\": 0.1,\n",
        "        \"epochs\": 5,\n",
        "        \"batch_size\": 12,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"label_smoothing\": 0.05,\n",
        "        \"dropout_factor\": 1.0,\n",
        "        \"freeze_layers\": [],\n",
        "        \"augmentation_strength\": \"light\"\n",
        "    },\n",
        "    \"layerwise\": {\n",
        "        \"name\": \"Layer-wise Fine-tuning\",\n",
        "        \"description\": \"Different learning rates for vision, text, and fusion components\",\n",
        "        \"learning_rate\": 2e-5,  # Increased from 5e-6 - better base learning rate\n",
        "        \"vision_lr_factor\": 0.1,  # Vision: 2e-6 (careful with pretrained weights)\n",
        "        \"text_lr_factor\": 0.5,   # Text: 1e-5 (moderate for LSTM)\n",
        "        \"fusion_lr_factor\": 1.0, # Fusion: 2e-5 (highest for new layers)\n",
        "        \"epochs\": 6,\n",
        "        \"batch_size\": 10,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"label_smoothing\": 0.1,\n",
        "        \"dropout_factor\": 1.0,\n",
        "        \"freeze_layers\": [],\n",
        "        \"augmentation_strength\": \"medium\"\n",
        "    },\n",
        "    \"progressive\": {\n",
        "        \"name\": \"Progressive Unfreezing\",\n",
        "        \"description\": \"Gradually unfreeze layers during training\",\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"vision_lr_factor\": 0.1,\n",
        "        \"epochs\": 8,\n",
        "        \"batch_size\": 12,\n",
        "        \"weight_decay\": 5e-5,\n",
        "        \"label_smoothing\": 0.1,\n",
        "        \"dropout_factor\": 1.0,\n",
        "        \"freeze_schedule\": {1: [\"vision_encoder.layer4\"], 3: [\"vision_encoder.layer3\"], 5: []},\n",
        "        \"augmentation_strength\": \"medium\"\n",
        "    },\n",
        "    \"aggressive\": {\n",
        "        \"name\": \"Aggressive Fine-tuning\",\n",
        "        \"description\": \"Higher learning rates for faster convergence\",\n",
        "        \"learning_rate\": 5e-5,  # Much higher learning rate\n",
        "        \"vision_lr_factor\": 0.2,  # Vision: 1e-5\n",
        "        \"text_lr_factor\": 0.8,   # Text: 4e-5\n",
        "        \"fusion_lr_factor\": 1.5,  # Fusion: 7.5e-5\n",
        "        \"epochs\": 4,\n",
        "        \"batch_size\": 8,\n",
        "        \"weight_decay\": 1e-5,\n",
        "        \"label_smoothing\": 0.05,\n",
        "        \"dropout_factor\": 0.8,\n",
        "        \"freeze_layers\": [],\n",
        "        \"augmentation_strength\": \"light\"\n",
        "    },\n",
        "    \"regularization\": {\n",
        "        \"name\": \"Regularization Tuning\",\n",
        "        \"description\": \"Optimize regularization parameters\",\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"vision_lr_factor\": 0.1,\n",
        "        \"epochs\": 6,\n",
        "        \"batch_size\": 12,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"label_smoothing\": 0.15,\n",
        "        \"dropout_factor\": 1.1,\n",
        "        \"freeze_layers\": [],\n",
        "        \"augmentation_strength\": \"medium\"\n",
        "    },\n",
        "    \"architecture\": {\n",
        "        \"name\": \"Architecture Tweaking\",\n",
        "        \"description\": \"Minor architectural modifications with optimal learning rates\",\n",
        "        \"learning_rate\": 1.5e-5,\n",
        "        \"vision_lr_factor\": 0.1,\n",
        "        \"epochs\": 6,\n",
        "        \"batch_size\": 10,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"label_smoothing\": 0.1,\n",
        "        \"dropout_factor\": 1.0,\n",
        "        \"freeze_layers\": [],\n",
        "        \"augmentation_strength\": \"medium\",\n",
        "        \"add_batch_norm\": True,\n",
        "        \"increase_attention_heads\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "config = FINE_TUNING_CONFIG[FINE_TUNING_STRATEGY]\n",
        "print(f\"Selected Strategy: {config['name']}\")\n",
        "print(f\"Description: {config['description']}\")\n",
        "print(f\"Epochs: {config['epochs']}, Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"Vision LR: {config['learning_rate'] * config['vision_lr_factor']:.2e}\")\n",
        "if 'text_lr_factor' in config:\n",
        "    print(f\"Text LR: {config['learning_rate'] * config['text_lr_factor']:.2e}\")\n",
        "    print(f\"Fusion LR: {config['learning_rate'] * config['fusion_lr_factor']:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0748aaf7",
      "metadata": {
        "id": "0748aaf7"
      },
      "source": [
        "## 3. Load Data with Enhanced Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b06226c",
      "metadata": {},
      "source": [
        "### üéØ Quick Strategy Selection\n",
        "\n",
        "**Available Strategies & When to Use:**\n",
        "\n",
        "1. **`layerwise`** ‚≠ê **(RECOMMENDED FIRST)** - Different LRs for model components\n",
        "   - Vision: 2e-6, Text: 1e-5, Fusion: 2e-5\n",
        "   - Best balance of stability and improvement\n",
        "\n",
        "2. **`aggressive`** - Higher learning rates for faster convergence\n",
        "   - Use if layerwise is too slow or conservative\n",
        "\n",
        "3. **`progressive`** - Gradually unfreeze layers\n",
        "   - Good for preventing catastrophic forgetting\n",
        "\n",
        "4. **`architecture`** - Add batch norm + more attention heads\n",
        "   - Try after finding good learning rates\n",
        "\n",
        "5. **`regularization`** - Optimize dropout and regularization\n",
        "   - Use if overfitting is an issue\n",
        "\n",
        "**Quick Switch:** Just change `FINE_TUNING_STRATEGY` above and re-run from cell 10 onwards!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "46891b33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46891b33",
        "outputId": "e5222e71-8703-4a03-bff4-d4edaa455e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: torch.utils.data._utils.pin_memory.pin_memory has been temporarily monkey-patched to disable pinning for debugging.\n",
            "         This allows data errors to be caught at a higher level. Remember to revert this patch for optimal performance!\n",
            "Loading data with light augmentation...\n",
            "Reduced vocabulary file not found, using full vocabulary\n",
            "Loaded 19755 samples\n",
            "  Vocab size: 5245\n",
            "  Num classes: 4142\n",
            "  Image size: 224x224\n",
            "Train size: 17780, Val size: 1975\n",
            "Loaded 3164 samples\n",
            "  Vocab size: 5245\n",
            "  Num classes: 4142\n",
            "  Image size: 224x224\n",
            "INFO: Explicitly disabled pin_memory for DataLoader instances.\n",
            "Data loaded: 1482 train, 165 val, 264 test batches\n",
            "Vocabulary size: 5245, Classes: 4142\n",
            "Using vocabulary file: answers.txt\n",
            "Note: Using standard augmentation from dataloader. Custom transforms defined but not applied yet.\n"
          ]
        }
      ],
      "source": [
        "# Temporary monkey-patch to disable pin_memory for debugging CUDA device-side assert triggered\n",
        "import torch.utils.data._utils.pin_memory as pin_memory_utils\n",
        "\n",
        "# Store original function to potentially restore later\n",
        "_original_pin_memory_fn = pin_memory_utils.pin_memory\n",
        "\n",
        "def no_op_pin_memory(data, device=None):\n",
        "    # This function simply returns the data, bypassing actual pinning\n",
        "    return data\n",
        "\n",
        "# Apply the monkey patch\n",
        "pin_memory_utils.pin_memory = no_op_pin_memory\n",
        "print(\"WARNING: torch.utils.data._utils.pin_memory.pin_memory has been temporarily monkey-patched to disable pinning for debugging.\")\n",
        "print(\"         This allows data errors to be caught at a higher level. Remember to revert this patch for optimal performance!\")\n",
        "\n",
        "# Load data with standard augmentation (the dataloader has its own transforms)\n",
        "data_dir = project_root / 'data'\n",
        "dataset_path = data_dir / 'train'\n",
        "\n",
        "print(f\"Loading data with {config['augmentation_strength']} augmentation...\")\n",
        "\n",
        "# Check if reduced vocabulary file exists, otherwise use full vocabulary\n",
        "answers_file_path = data_dir / 'answers_top_1000.txt'\n",
        "if not answers_file_path.exists():\n",
        "    print(\"Reduced vocabulary file not found, using full vocabulary\")\n",
        "    answers_file_path = data_dir / 'answers.txt'\n",
        "\n",
        "# Use standard dataloader without custom transforms (it has built-in augmentation)\n",
        "train_loader, val_loader, test_loader, vocab_size, num_classes, vocab, answer_to_idx = create_multimodal_dataloaders(\n",
        "    train_csv=str(data_dir / 'trainrenamed.csv'),\n",
        "    test_csv=str(data_dir / 'testrenamed.csv'),\n",
        "    image_dir=str(dataset_path),\n",
        "    answers_file=str(answers_file_path),\n",
        "    batch_size=config['batch_size'],\n",
        "    val_split=0.1,\n",
        "    num_workers=0,\n",
        "    image_size=224\n",
        ")\n",
        "\n",
        "# Explicitly disable pin_memory on the DataLoader instances to prevent CUDA asserts\n",
        "# (This is a more robust fix than just monkey-patching torch.utils.data._utils.pin_memory.pin_memory if DataLoader was already instantiated)\n",
        "if hasattr(train_loader, 'pin_memory'):\n",
        "    train_loader.pin_memory = False\n",
        "    val_loader.pin_memory = False\n",
        "    test_loader.pin_memory = False\n",
        "    print(\"INFO: Explicitly disabled pin_memory for DataLoader instances.\")\n",
        "\n",
        "print(f\"Data loaded: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches\")\n",
        "print(f\"Vocabulary size: {vocab_size}, Classes: {num_classes}\")\n",
        "print(f\"Using vocabulary file: {answers_file_path.name}\")\n",
        "print(\"Note: Using standard augmentation from dataloader. Custom transforms defined but not applied yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f030b7ca",
      "metadata": {
        "id": "f030b7ca"
      },
      "source": [
        "## 4. Load Best Model and Create Fine-tuned Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6c89d0a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c89d0a4",
        "outputId": "d19d4e87-f365-44e4-eaa6-49a27a40a07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/WOA7015 Advanced Machine Learning/checkpoints/multimodal_concat/best_model.pth\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 195MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Original best validation accuracy from checkpoint: N/A\n",
            "Total parameters: 33,285,611\n",
            "Trainable parameters: 33,285,611\n"
          ]
        }
      ],
      "source": [
        "# The model is already imported at the top of the notebook\n",
        "\n",
        "def create_fine_tuned_model(base_model, config):\n",
        "    \"\"\"Create a fine-tuned version of the model with optional architectural tweaks\"\"\"\n",
        "\n",
        "    if config.get('add_batch_norm', False):\n",
        "        # Add batch normalization layers\n",
        "        print(\"Adding batch normalization to classifier\")\n",
        "        base_model.classifier = nn.Sequential(\n",
        "            nn.Linear(base_model.classifier[0].in_features, base_model.classifier[0].out_features),\n",
        "            nn.BatchNorm1d(base_model.classifier[0].out_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(base_model.classifier[2].p * config.get('dropout_factor', 1.0)),\n",
        "            nn.Linear(base_model.classifier[4].in_features, base_model.classifier[4].out_features)\n",
        "        )\n",
        "\n",
        "    if config.get('increase_attention_heads', False):\n",
        "        # Increase attention heads if possible\n",
        "        print(\"Increasing attention heads to 16\")\n",
        "        base_model.cross_attention = nn.MultiheadAttention(\n",
        "            embed_dim=512,\n",
        "            num_heads=16,  # Increased from 8\n",
        "            dropout=0.3 * config.get('dropout_factor', 1.0)\n",
        "        )\n",
        "\n",
        "    # Adjust dropout in other layers if needed\n",
        "    if config.get('dropout_factor', 1.0) != 1.0:\n",
        "        factor = config['dropout_factor']\n",
        "        base_model.text_dropout.p = min(0.5, base_model.text_dropout.p * factor)\n",
        "\n",
        "        # Update classifier dropout if not already modified\n",
        "        if not config.get('add_batch_norm', False):\n",
        "            base_model.classifier[2].p = min(0.7, base_model.classifier[2].p * factor)\n",
        "\n",
        "    return base_model\n",
        "\n",
        "# Load the best checkpoint\n",
        "checkpoint_dir = project_root / 'checkpoints' / 'multimodal_concat'\n",
        "checkpoint_path = checkpoint_dir / 'best_model.pth'\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    print(f\"Loading best model from {checkpoint_path}\")\n",
        "\n",
        "    # Create base model\n",
        "    model = ImprovedMultimodalVQA(\n",
        "        vocab_size=vocab_size,\n",
        "        num_classes=num_classes,\n",
        "        embedding_dim=300,\n",
        "        text_hidden_dim=512,\n",
        "        fusion_hidden_dim=512,\n",
        "        dropout=0.3\n",
        "    )\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "\n",
        "    # Apply fine-tuning modifications\n",
        "    model = create_fine_tuned_model(model, config)\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(f\"Model loaded successfully!\")\n",
        "    print(f\"Original best validation accuracy from checkpoint: {checkpoint.get('best_val_acc', 'N/A')}\")\n",
        "\n",
        "    # Print model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Best model checkpoint not found at {checkpoint_path}\")\n",
        "    print(\"Please run the main training notebook first to create the best model checkpoint.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420b9548",
      "metadata": {
        "id": "420b9548"
      },
      "source": [
        "## 5. Setup Fine-tuning Optimizer and Training Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "be510bb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be510bb4",
        "outputId": "d2d83552-f28f-4918-945f-2de87c5c682c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vision parameters: 165\n",
            "Text parameters: 11\n",
            "Fusion parameters: 8\n",
            "Standard LR: vision=1.00e-07, other=1.00e-06\n",
            "Using label smoothing: 0.05\n",
            "Setup complete for Conservative Fine-tuning strategy\n"
          ]
        }
      ],
      "source": [
        "def setup_fine_tuning_optimizer(model, config):\n",
        "    \"\"\"Setup optimizer with strategy-specific parameters\"\"\"\n",
        "\n",
        "    # Categorize parameters\n",
        "    vision_params = []\n",
        "    text_params = []\n",
        "    fusion_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(x in name for x in ['vision_encoder', 'spatial_attention', 'vision_proj']):\n",
        "            vision_params.append(param)\n",
        "        elif any(x in name for x in ['text_embedding', 'text_lstm', 'text_proj', 'text_dropout']):\n",
        "            text_params.append(param)\n",
        "        else:\n",
        "            fusion_params.append(param)\n",
        "\n",
        "    print(f\"Vision parameters: {len(vision_params)}\")\n",
        "    print(f\"Text parameters: {len(text_params)}\")\n",
        "    print(f\"Fusion parameters: {len(fusion_params)}\")\n",
        "\n",
        "    # Setup parameter groups based on strategy\n",
        "    if FINE_TUNING_STRATEGY == \"layerwise\":\n",
        "        param_groups = [\n",
        "            {'params': vision_params, 'lr': config['learning_rate'] * config['vision_lr_factor'], 'name': 'vision'},\n",
        "            {'params': text_params, 'lr': config['learning_rate'] * config.get('text_lr_factor', 0.3), 'name': 'text'},\n",
        "            {'params': fusion_params, 'lr': config['learning_rate'] * config.get('fusion_lr_factor', 1.0), 'name': 'fusion'}\n",
        "        ]\n",
        "        print(f\"Layer-wise LR: vision={param_groups[0]['lr']:.2e}, text={param_groups[1]['lr']:.2e}, fusion={param_groups[2]['lr']:.2e}\")\n",
        "    else:\n",
        "        param_groups = [\n",
        "            {'params': vision_params, 'lr': config['learning_rate'] * config['vision_lr_factor'], 'name': 'vision'},\n",
        "            {'params': text_params + fusion_params, 'lr': config['learning_rate'], 'name': 'other'}\n",
        "        ]\n",
        "        print(f\"Standard LR: vision={param_groups[0]['lr']:.2e}, other={param_groups[1]['lr']:.2e}\")\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups, weight_decay=config['weight_decay'])\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def apply_layer_freezing(model, freeze_layers):\n",
        "    \"\"\"Freeze specified layers\"\"\"\n",
        "    for layer_name in freeze_layers:\n",
        "        for name, param in model.named_parameters():\n",
        "            if layer_name in name:\n",
        "                param.requires_grad = False\n",
        "                print(f\"Frozen: {name}\")\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = setup_fine_tuning_optimizer(model, config)\n",
        "\n",
        "# Apply initial layer freezing if specified\n",
        "if config.get('freeze_layers', []):\n",
        "    apply_layer_freezing(model, config['freeze_layers'])\n",
        "\n",
        "# Setup loss function with label smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
        "print(f\"Using label smoothing: {config['label_smoothing']}\")\n",
        "\n",
        "# Setup scheduler\n",
        "if FINE_TUNING_STRATEGY == \"progressive\":\n",
        "    scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.5, total_iters=config['epochs']//2)\n",
        "else:\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=max(2, config['epochs']//3), T_mult=2, eta_min=1e-7\n",
        "    )\n",
        "\n",
        "print(f\"Setup complete for {config['name']} strategy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a77a4c",
      "metadata": {
        "id": "d4a77a4c"
      },
      "source": [
        "## 6. Fine-tuning Training Loop with Advanced Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "26a108f1",
      "metadata": {
        "id": "26a108f1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def _validate_and_process_tensor_cpu(\n",
        "    tensor,\n",
        "    target_dtype,\n",
        "    tensor_name,\n",
        "    vocab_size_check=None,\n",
        "    num_classes_check=None\n",
        "):\n",
        "    if tensor is None or not torch.is_tensor(tensor):\n",
        "        return None\n",
        "\n",
        "    # NaN / Inf checks\n",
        "    if torch.is_floating_point(tensor):\n",
        "        if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
        "            print(f\"ERROR: Invalid NaN/Inf in {tensor_name}\")\n",
        "            return None\n",
        "\n",
        "    if target_dtype == \"long\":\n",
        "        if tensor.min().item() < 0:\n",
        "            print(f\"ERROR: Negative index in {tensor_name}\")\n",
        "            return None\n",
        "\n",
        "        if tensor_name == \"questions\" and vocab_size_check is not None:\n",
        "            if tensor.max().item() >= vocab_size_check:\n",
        "                print(f\"ERROR: Question ID out of bounds\")\n",
        "                return None\n",
        "\n",
        "        if tensor_name == \"answers\" and num_classes_check is not None:\n",
        "            if tensor.max().item() >= num_classes_check:\n",
        "                print(f\"ERROR: Answer ID out of bounds\")\n",
        "                return None\n",
        "\n",
        "        return tensor.long()\n",
        "\n",
        "    if target_dtype == \"float\":\n",
        "        return tensor.float()\n",
        "\n",
        "    # attention mask handling\n",
        "    if tensor_name == \"attention_mask\" and tensor.dtype == torch.bool:\n",
        "        return tensor.long()\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def _safe_get(batch, *keys):\n",
        "    \"\"\"Safely retrieve first existing non-None key from dict\"\"\"\n",
        "    for k in keys:\n",
        "        if k in batch and batch[k] is not None:\n",
        "            return batch[k]\n",
        "    return None\n",
        "\n",
        "\n",
        "def _extract_and_validate_batch_cpu(batch, vocab_size, num_classes):\n",
        "    questions = attention_mask = images = answers = None\n",
        "\n",
        "    if isinstance(batch, dict):\n",
        "        questions = _safe_get(batch, \"question\", \"input_ids\", \"questions\")\n",
        "        attention_mask = _safe_get(batch, \"attention_mask\", \"mask\")\n",
        "        images = _safe_get(batch, \"image\", \"images\", \"pixel_values\")\n",
        "        answers = _safe_get(batch, \"answer\", \"answers\", \"labels\", \"target\")\n",
        "\n",
        "    elif isinstance(batch, (list, tuple)):\n",
        "        for item in batch:\n",
        "            if not torch.is_tensor(item):\n",
        "                continue\n",
        "\n",
        "            if item.ndim == 4 and item.dtype == torch.float:\n",
        "                images = images or item\n",
        "            elif item.ndim == 2 and item.dtype == torch.long:\n",
        "                questions = questions or item\n",
        "            elif item.ndim == 2 and item.dtype in (torch.bool, torch.uint8, torch.float):\n",
        "                attention_mask = attention_mask or item\n",
        "            elif item.ndim == 1 and item.dtype == torch.long:\n",
        "                answers = answers or item\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    questions = _validate_and_process_tensor_cpu(\n",
        "        questions, \"long\", \"questions\", vocab_size, num_classes\n",
        "    )\n",
        "    attention_mask = _validate_and_process_tensor_cpu(\n",
        "        attention_mask, None, \"attention_mask\"\n",
        "    )\n",
        "    images = _validate_and_process_tensor_cpu(\n",
        "        images, \"float\", \"images\"\n",
        "    )\n",
        "    answers = _validate_and_process_tensor_cpu(\n",
        "        answers, \"long\", \"answers\", None, num_classes\n",
        "    )\n",
        "\n",
        "    if questions is None or images is None or answers is None:\n",
        "        return None\n",
        "\n",
        "    # Safe shape normalization\n",
        "    if questions.ndim == 3 and questions.size(1) == 1:\n",
        "        questions = questions.squeeze(1)\n",
        "\n",
        "    if images.ndim == 5 and images.size(1) == 1:\n",
        "        images = images.squeeze(1)\n",
        "\n",
        "    if questions.ndim not in (2, 3):\n",
        "        return None\n",
        "\n",
        "    return questions, attention_mask, images, answers\n",
        "\n",
        "\n",
        "def fine_tune_epoch(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epoch,\n",
        "    config,\n",
        "    vocab_size,\n",
        "    num_classes\n",
        "):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{config['epochs']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        batch_data = _extract_and_validate_batch_cpu(\n",
        "            batch, vocab_size, num_classes\n",
        "        )\n",
        "        if batch_data is None:\n",
        "            continue\n",
        "\n",
        "        questions, attention_mask, images, answers = batch_data\n",
        "\n",
        "        questions = questions.to(device)\n",
        "        images = images.to(device)\n",
        "        answers = answers.to(device)\n",
        "        attention_mask = attention_mask.to(device) if attention_mask is not None else None\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            outputs = (\n",
        "                model(questions, images, attention_mask=attention_mask)\n",
        "                if attention_mask is not None\n",
        "                else model(questions, images)\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Forward error: {e}\")\n",
        "            continue\n",
        "\n",
        "        if outputs.ndim != 2 or outputs.size(1) != num_classes:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs, answers)\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_total += answers.size(0)\n",
        "        train_correct += (preds == answers).sum().item()\n",
        "\n",
        "    if not train_losses:\n",
        "        return dict(\n",
        "            train_loss=float(\"inf\"),\n",
        "            train_accuracy=0.0,\n",
        "            val_loss=float(\"inf\"),\n",
        "            val_accuracy=0.0,\n",
        "        )\n",
        "\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "    train_accuracy = 100.0 * train_correct / train_total\n",
        "\n",
        "    # ================= VALIDATION =================\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            batch_data = _extract_and_validate_batch_cpu(\n",
        "                batch, vocab_size, num_classes\n",
        "            )\n",
        "            if batch_data is None:\n",
        "                continue\n",
        "\n",
        "            questions, attention_mask, images, answers = batch_data\n",
        "\n",
        "            questions = questions.to(device)\n",
        "            images = images.to(device)\n",
        "            answers = answers.to(device)\n",
        "            attention_mask = attention_mask.to(device) if attention_mask is not None else None\n",
        "\n",
        "            try:\n",
        "                outputs = (\n",
        "                    model(questions, images, attention_mask=attention_mask)\n",
        "                    if attention_mask is not None\n",
        "                    else model(questions, images)\n",
        "                )\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            if outputs.ndim != 2 or outputs.size(1) != num_classes:\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, answers)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_total += answers.size(0)\n",
        "            val_correct += (preds == answers).sum().item()\n",
        "\n",
        "    avg_val_loss = sum(val_losses) / len(val_losses) if val_losses else float(\"inf\")\n",
        "    val_accuracy = 100.0 * val_correct / val_total if val_total else 0.0\n",
        "\n",
        "    if scheduler is not None:\n",
        "        try:\n",
        "            scheduler.step(avg_val_loss)\n",
        "        except TypeError:\n",
        "            scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
        "    print(f\"Val   Loss: {avg_val_loss:.4f} | Val   Acc: {val_accuracy:.2f}%\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    return dict(\n",
        "        train_loss=avg_train_loss,\n",
        "        train_accuracy=train_accuracy,\n",
        "        val_loss=avg_val_loss,\n",
        "        val_accuracy=val_accuracy,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986aae86",
      "metadata": {
        "id": "986aae86"
      },
      "source": [
        "\n",
        "## 7. Execute Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5c7b38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac5c7b38",
        "outputId": "2d6af78c-15b5-4c70-d0ff-2e23b3a0cd82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Starting Fine-tuning (Conservative Fine-tuning) ====================\n",
            "\n",
            "Epoch 1/5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1482/1482 [09:00<00:00,  2.74it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:45<00:00,  3.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 6.2360 | Train Acc: 27.13%\n",
            "Val   Loss: 5.7744 | Val   Acc: 24.96%\n",
            "==================================================\n",
            "--> Saved best model with validation accuracy: 24.96%\n",
            "\n",
            "Epoch 2/5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1482/1482 [09:00<00:00,  2.74it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:46<00:00,  3.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 5.8915 | Train Acc: 26.11%\n",
            "Val   Loss: 5.7027 | Val   Acc: 24.96%\n",
            "==================================================\n",
            "\n",
            "Epoch 3/5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1482/1482 [08:59<00:00,  2.75it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:46<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 5.8600 | Train Acc: 25.77%\n",
            "Val   Loss: 5.6688 | Val   Acc: 24.96%\n",
            "==================================================\n",
            "\n",
            "Epoch 4/5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|‚ñà‚ñé        | 190/1482 [01:09<07:31,  2.86it/s]"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*20} Starting Fine-tuning ({config['name']}) {'='*20}\")\n",
        "\n",
        "fine_tune_history = {\n",
        "    'train_losses': [],\n",
        "    'train_accuracies': [],\n",
        "    'val_losses': [],\n",
        "    'val_accuracies': []\n",
        "}\n",
        "\n",
        "best_fine_tune_acc = 0.0\n",
        "ft_checkpoint_path = checkpoint_dir / f\"fine_tuned_best_model_{FINE_TUNING_STRATEGY}.pth\"\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, config['epochs'] + 1):\n",
        "\n",
        "    epoch_results = fine_tune_epoch(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        epoch=epoch,\n",
        "        config=config,\n",
        "        vocab_size=vocab_size,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "\n",
        "    fine_tune_history['train_losses'].append(epoch_results['train_loss'])\n",
        "    fine_tune_history['train_accuracies'].append(epoch_results['train_accuracy'])\n",
        "    fine_tune_history['val_losses'].append(epoch_results['val_loss'])\n",
        "    fine_tune_history['val_accuracies'].append(epoch_results['val_accuracy'])\n",
        "\n",
        "    # Save best model\n",
        "    if epoch_results['val_accuracy'] > best_fine_tune_acc:\n",
        "        best_fine_tune_acc = epoch_results['val_accuracy']\n",
        "\n",
        "        torch.save(\n",
        "            {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_fine_tune_acc,\n",
        "                'config': config,\n",
        "                'fine_tuning_strategy': FINE_TUNING_STRATEGY,\n",
        "            },\n",
        "            ft_checkpoint_path\n",
        "        )\n",
        "\n",
        "        print(f\"--> Saved best model with validation accuracy: {best_fine_tune_acc:.2f}%\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n{'='*20} Fine-tuning Complete {'='*20}\")\n",
        "print(f\"Total Fine-tuning Time: {training_time / 60:.2f} minutes\")\n",
        "print(f\"Best Validation Accuracy Achieved: {best_fine_tune_acc:.2f}%\")\n",
        "print(f\"Best model saved to: {ft_checkpoint_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e3158d1",
      "metadata": {
        "id": "3e3158d1"
      },
      "source": [
        "## 8. Comprehensive Evaluation and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259e65f3",
      "metadata": {
        "id": "259e65f3"
      },
      "outputs": [],
      "source": [
        "# Comprehensive evaluation function\n",
        "def evaluate_fine_tuned_model(model, test_loader, device, strategy_name, vocab_size, num_classes):\n",
        "    \"\"\"Comprehensive evaluation of fine-tuned model\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    test_loss = 0.0\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"Evaluating {strategy_name} fine-tuned model on test set...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(test_loader, desc=\"Testing\")\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            _SKIP_BATCH = False\n",
        "            questions_cpu, attention_mask_cpu, images_cpu, answers_cpu = None, None, None, None\n",
        "\n",
        "            # Attempt to extract tensors flexibly (on CPU first)\n",
        "            try:\n",
        "                if isinstance(batch, dict):\n",
        "                    questions_raw = batch.get('question') or batch.get('input_ids') or batch.get('questions')\n",
        "                    attention_mask_raw = batch.get('attention_mask') or batch.get('mask')\n",
        "                    images_raw = batch.get('image') or batch.get('images') or batch.get('pixel_values')\n",
        "                    answers_raw = batch.get('answer') or batch.get('answers') or batch.get('labels') or batch.get('target')\n",
        "\n",
        "                    questions_cpu = _validate_and_process_tensor_cpu(questions_raw, 'long', 'questions', vocab_size, num_classes)\n",
        "                    attention_mask_cpu = _validate_and_process_tensor_cpu(attention_mask_raw, None, 'attention_mask', vocab_size, num_classes)\n",
        "                    images_cpu = _validate_and_process_tensor_cpu(images_raw, 'float', 'images', vocab_size, num_classes)\n",
        "                    answers_cpu = _validate_and_process_tensor_cpu(answers_raw, 'long', 'answers', vocab_size, num_classes)\n",
        "\n",
        "                elif isinstance(batch, (list, tuple)):\n",
        "                    found_questions = None\n",
        "                    found_attention_mask = None\n",
        "                    found_images = None\n",
        "                    found_answers = None\n",
        "\n",
        "                    for item in batch:\n",
        "                        if not torch.is_tensor(item):\n",
        "                            continue # Skip non-tensor items\n",
        "\n",
        "                        if item.ndim == 4 and item.dtype == torch.float: # Likely images (Batch, C, H, W)\n",
        "                            if found_images is None: found_images = item\n",
        "                        elif item.ndim == 2 and item.dtype == torch.long: # Likely questions (Batch, SeqLen)\n",
        "                            if found_questions is None: found_questions = item\n",
        "                        elif item.ndim == 2 and (item.dtype == torch.bool or item.dtype == torch.uint8 or item.dtype == torch.float): # Likely attention mask\n",
        "                            if found_attention_mask is None: found_attention_mask = item\n",
        "                        elif item.ndim == 1 and item.dtype == torch.long: # Likely answers (Batch)\n",
        "                            if found_answers is None: found_answers = item\n",
        "\n",
        "                    questions_cpu = _validate_and_process_tensor_cpu(found_questions, 'long', 'questions', vocab_size, num_classes)\n",
        "                    attention_mask_cpu = _validate_and_process_tensor_cpu(found_attention_mask, None, 'attention_mask', vocab_size, num_classes)\n",
        "                    images_cpu = _validate_and_process_tensor_cpu(found_images, 'float', 'images', vocab_size, num_classes)\n",
        "                    answers_cpu = _validate_and_process_tensor_cpu(found_answers, 'long', 'answers', vocab_size, num_classes)\n",
        "\n",
        "                # If any CPU validation failed, skip batch\n",
        "                if questions_cpu is None or images_cpu is None or answers_cpu is None:\n",
        "                    _SKIP_BATCH = True\n",
        "\n",
        "                if not _SKIP_BATCH:\n",
        "                    # Safe squeeze operations on CPU tensors\n",
        "                    questions_cpu = questions_cpu.squeeze() if questions_cpu.ndim > 2 else questions_cpu\n",
        "                    images_cpu = images_cpu.squeeze() if images_cpu.ndim > 4 else images_cpu\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Exception during batch unpacking/CPU validation at evaluation batch {batch_idx}: {e}\")\n",
        "                print(f\"Batch content type: {type(batch)}\")\n",
        "                if isinstance(batch, dict): print(f\"Batch keys: {batch.keys()}\")\n",
        "                else: print(f\"Batch length: {len(batch) if hasattr(batch, '__len__') else 'N/A'}\")\n",
        "                _SKIP_BATCH = True\n",
        "\n",
        "            if _SKIP_BATCH:\n",
        "                continue\n",
        "\n",
        "            # --- Move validated CPU tensors to device ---\n",
        "            questions = questions_cpu.to(device)\n",
        "            attention_mask = attention_mask_cpu.to(device) if attention_mask_cpu is not None else None\n",
        "            images = images_cpu.to(device)\n",
        "            answers = answers_cpu.to(device)\n",
        "\n",
        "            # Ensure questions tensor is the right shape for LSTM (2D or 3D)\n",
        "            if questions.ndim != 2 and questions.ndim != 3:\n",
        "                print(f\"ERROR: Questions tensor has {questions.ndim} dimensions (expected 2 or 3) at evaluation batch {batch_idx} after squeeze. Shape: {questions.shape}\")\n",
        "                _SKIP_BATCH = True\n",
        "\n",
        "            if _SKIP_BATCH:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                if attention_mask is not None:\n",
        "                    outputs = model(questions, images, attention_mask=attention_mask)\n",
        "                else:\n",
        "                    outputs = model(questions, images)\n",
        "            except TypeError:\n",
        "                outputs = model(questions, images)\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Exception during model forward pass in evaluation batch {batch_idx}: {e}\")\n",
        "                _SKIP_BATCH = True\n",
        "            if _SKIP_BATCH:\n",
        "                continue\n",
        "\n",
        "            # Validate model outputs\n",
        "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
        "                print(f\"ERROR: Invalid values in model outputs at evaluation batch {batch_idx}\")\n",
        "                _SKIP_BATCH = True\n",
        "            if outputs.shape[1] != num_classes:\n",
        "                print(f\"ERROR: Model output shape mismatch: {outputs.shape[1]} vs {num_classes} at evaluation batch {batch_idx}\")\n",
        "                _SKIP_BATCH = True\n",
        "            if _SKIP_BATCH:\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, answers)\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"ERROR: Invalid loss value at evaluation batch {batch_idx}: {loss}\")\n",
        "                _SKIP_BATCH = True\n",
        "            if _SKIP_BATCH:\n",
        "                continue\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(answers.cpu().numpy())\n",
        "\n",
        "    # Check if any batches were processed successfully\n",
        "    if len(test_loader) == 0 or len(all_predictions) == 0:\n",
        "        print(\"WARNING: No valid batches processed during evaluation. Returning default metrics.\")\n",
        "        return {\n",
        "            'accuracy': 0.0,\n",
        "            'avg_loss': float('inf'),\n",
        "            'predictions': [],\n",
        "            'targets': []\n",
        "        }\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'avg_loss': avg_loss,\n",
        "        'predictions': all_predictions,\n",
        "        'targets': all_targets\n",
        "    }\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "ft_results = evaluate_fine_tuned_model(model, test_loader, device, config['name'], vocab_size, num_classes)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"FINE-TUNING RESULTS - {config['name'].upper()}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test Accuracy: {ft_results['accuracy']:.4f} ({ft_results['accuracy']*100:.2f}%)\")\n",
        "print(f\"Test Loss: {ft_results['avg_loss']:.4f}\")\n",
        "\n",
        "# Load original results for comparison\n",
        "baseline_results_path = project_root / 'results' / 'text_baseline_results.json'\n",
        "original_multimodal_path = project_root / 'results' / 'improved_multimodal_results.json'\n",
        "\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if baseline_results_path.exists():\n",
        "    with open(baseline_results_path, 'r') as f:\n",
        "        baseline_results = json.load(f)\n",
        "    baseline_acc = baseline_results['test_accuracy'] # Corrected key\n",
        "    print(f\"Text Baseline:        {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
        "\n",
        "    improvement_vs_baseline = (ft_results['accuracy'] - baseline_acc) * 100\n",
        "    print(f\"vs Text Baseline:     {improvement_vs_baseline:+.2f} pp\")\n",
        "\n",
        "if original_multimodal_path.exists():\n",
        "    with open(original_multimodal_path, 'r') as f:\n",
        "        original_results = json.load(f)\n",
        "    original_acc = original_results['test_metrics']['accuracy']\n",
        "    print(f\"Original Multimodal:  {original_acc:.4f} ({original_acc*100:.2f}%)\")\n",
        "\n",
        "    improvement_vs_original = (ft_results['accuracy'] - original_acc) * 100\n",
        "    print(f\"vs Original:          {improvement_vs_original:+.2f} pp\")\n",
        "\n",
        "print(f\"Fine-tuned Model:     {ft_results['accuracy']:.4f} ({ft_results['accuracy']*100:.2f}%)\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ca9f51",
      "metadata": {
        "id": "c9ca9f51"
      },
      "source": [
        "## 9. Visualization and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730fad22",
      "metadata": {
        "id": "730fad22"
      },
      "outputs": [],
      "source": [
        "# Training history visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "epochs_range = range(1, len(fine_tune_history['train_losses']) + 1)\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(epochs_range, fine_tune_history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
        "ax1.plot(epochs_range, fine_tune_history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
        "ax1.set_title(f'Fine-tuning Loss Curves - {config[\"name\"]}', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "ax2.plot(epochs_range, fine_tune_history['train_accuracies'], 'b-', label='Training Accuracy', linewidth=2)\n",
        "ax2.plot(epochs_range, fine_tune_history['val_accuracies'], 'r-', label='Validation Accuracy', linewidth=2)\n",
        "ax2.axhline(y=original_best_acc, color='g', linestyle='--', alpha=0.7, label=f'Original Best ({original_best_acc:.1f}%)')\n",
        "ax2.set_title('Fine-tuning Accuracy Curves', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Performance comparison bar chart\n",
        "models = ['Text Baseline', 'Original Multimodal', f'Fine-tuned\\n({FINE_TUNING_STRATEGY})']\n",
        "accuracies = []\n",
        "\n",
        "if baseline_results_path.exists():\n",
        "    accuracies.append(baseline_acc * 100)\n",
        "else:\n",
        "    accuracies.append(47.36)  # Known baseline\n",
        "\n",
        "if original_multimodal_path.exists():\n",
        "    accuracies.append(original_acc * 100)\n",
        "else:\n",
        "    accuracies.append(55.39)  # Known best\n",
        "\n",
        "accuracies.append(ft_results['accuracy'] * 100)\n",
        "\n",
        "bars = ax3.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
        "            f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax3.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Accuracy (%)')\n",
        "ax3.set_ylim(0, max(accuracies) + 5)\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Fine-tuning strategy summary\n",
        "ax4.axis('off')\n",
        "summary_text = f\"\"\"\n",
        "Fine-tuning Strategy: {config['name']}\n",
        "\n",
        "Configuration:\n",
        "‚Ä¢ Learning Rate: {config['learning_rate']:.2e}\n",
        "‚Ä¢ Vision LR Factor: {config['vision_lr_factor']}\n",
        "‚Ä¢ Epochs: {config['epochs']}\n",
        "‚Ä¢ Label Smoothing: {config['label_smoothing']}\n",
        "‚Ä¢ Augmentation: {config['augmentation_strength']}\n",
        "\n",
        "Results:\n",
        "‚Ä¢ Best Val Acc: {best_fine_tune_acc:.2f}%\n",
        "‚Ä¢ Test Accuracy: {ft_results['accuracy']*100:.2f}%\n",
        "‚Ä¢ Improvement: {improvement:+.2f} pp\n",
        "‚Ä¢ Training Time: {training_time/60:.1f} min\n",
        "\"\"\"\n",
        "\n",
        "ax4.text(0.05, 0.95, summary_text, fontsize=11, verticalalignment='top',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.7))\n",
        "ax4.set_title('Fine-tuning Summary', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save visualization\n",
        "results_dir = project_root / 'results' / 'figures'\n",
        "results_dir.mkdir(exist_ok=True, parents=True)\n",
        "fig.savefig(results_dir / f'fine_tuning_{FINE_TUNING_STRATEGY}_results.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"\\nVisualization saved to: {results_dir / f'fine_tuning_{FINE_TUNING_STRATEGY}_results.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067d3f4b",
      "metadata": {
        "id": "067d3f4b"
      },
      "source": [
        "## 10. Save Fine-tuning Results and Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dfdd990",
      "metadata": {
        "id": "3dfdd990"
      },
      "outputs": [],
      "source": [
        "# Save comprehensive fine-tuning results\n",
        "results_dir = project_root / 'results'\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "fine_tuning_results = {\n",
        "    'strategy': FINE_TUNING_STRATEGY,\n",
        "    'strategy_name': config['name'],\n",
        "    'strategy_description': config['description'],\n",
        "    'configuration': config,\n",
        "    'training_history': fine_tune_history,\n",
        "    'results': {\n",
        "        'best_validation_accuracy': best_fine_tune_acc / 100,\n",
        "        'test_accuracy': ft_results['accuracy'],\n",
        "        'test_loss': ft_results['avg_loss'],\n",
        "        'training_time_minutes': training_time / 60,\n",
        "        'total_epochs': len(fine_tune_history['train_losses'])\n",
        "    },\n",
        "    'improvements': {\n",
        "        'vs_original_multimodal': improvement,\n",
        "        'vs_text_baseline': improvement_vs_baseline if 'improvement_vs_baseline' in locals() else None\n",
        "    },\n",
        "    'model_info': {\n",
        "        'total_parameters': total_params,\n",
        "        'trainable_parameters': trainable_params\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results\n",
        "results_file = results_dir / f'fine_tuning_{FINE_TUNING_STRATEGY}_results.json'\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(fine_tuning_results, f, indent=2)\n",
        "\n",
        "print(f\"Fine-tuning results saved to: {results_file}\")\n",
        "\n",
        "# Generate confusion matrix if we have good performance\n",
        "if ft_results['accuracy'] > 0.5:  # Only if accuracy is decent\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Use a subset of classes for cleaner visualization\n",
        "    unique_targets = sorted(set(ft_results['targets']))\n",
        "    if len(unique_targets) > 20:  # Too many classes for clean visualization\n",
        "        # Show top 20 most common classes\n",
        "        from collections import Counter\n",
        "        target_counts = Counter(ft_results['targets'])\n",
        "        top_classes = [cls for cls, _ in target_counts.most_common(20)]\n",
        "\n",
        "        # Filter data for top classes only\n",
        "        filtered_targets = []\n",
        "        filtered_preds = []\n",
        "        for i, target in enumerate(ft_results['targets']):\n",
        "            if target in top_classes:\n",
        "                filtered_targets.append(target)\n",
        "                filtered_preds.append(ft_results['predictions'][i])\n",
        "\n",
        "        cm = confusion_matrix(filtered_targets, filtered_preds, labels=top_classes)\n",
        "        title_suffix = \" (Top 20 Classes)\"\n",
        "    else:\n",
        "        cm = confusion_matrix(ft_results['targets'], ft_results['predictions'])\n",
        "        title_suffix = \"\"\n",
        "\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', cbar=True)\n",
        "    plt.title(f'Fine-tuned Model Confusion Matrix{title_suffix}\\n{config[\"name\"]}')\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('True Class')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save confusion matrix\n",
        "    cm_path = results_dir / 'figures' / f'fine_tuned_{FINE_TUNING_STRATEGY}_confusion_matrix.png'\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Confusion matrix saved to: {cm_path}\")\n",
        "\n",
        "# Print final summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"FINE-TUNING COMPLETED: {config['name'].upper()}\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Strategy Description: {config['description']}\")\n",
        "print(f\"Training Time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Best Validation Accuracy: {best_fine_tune_acc:.2f}%\")\n",
        "print(f\"Test Accuracy: {ft_results['accuracy']*100:.2f}%\")\n",
        "print(f\"Improvement over Original: {improvement:+.2f} percentage points\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(\"\\nCONGRATULATIONS! Fine-tuning was successful!\")\n",
        "    print(f\"Your model has improved by {improvement:.2f} percentage points.\")\n",
        "else:\n",
        "    print(\"\\nFine-tuning did not improve performance.\")\n",
        "    print(\"Consider trying a different strategy or adjusting hyperparameters.\")\n",
        "\n",
        "print(f\"\\nAll results and checkpoints saved in:\")\n",
        "print(f\"- Results: {results_file}\")\n",
        "print(f\"- Checkpoint: {ft_checkpoint_path}\")\n",
        "print(f\"- Visualizations: {results_dir / 'figures'}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a1f7a37",
      "metadata": {
        "id": "8a1f7a37"
      },
      "source": [
        "## 11. Strategy Comparison and Recommendations\n",
        "\n",
        "**Available Fine-tuning Strategies:**\n",
        "\n",
        "1. **Conservative:** Safe, minimal changes with very low learning rates\n",
        "2. **Layerwise:** Different learning rates for different model components  \n",
        "3. **Progressive:** Gradually unfreeze layers during training\n",
        "4. **Augmented:** Enhanced data augmentation for better generalization\n",
        "5. **Regularization:** Optimize dropout and weight decay parameters\n",
        "6. **Architecture:** Minor architectural modifications\n",
        "\n",
        "**To try different strategies:**\n",
        "1. Change `FINE_TUNING_STRATEGY` at the top of this notebook\n",
        "2. Re-run cells 2 onwards\n",
        "3. Compare results across strategies\n",
        "\n",
        "**Next Steps:**\n",
        "- Try multiple strategies and compare results\n",
        "- Ensemble the best fine-tuned models\n",
        "- Use the best model for inference applications"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
