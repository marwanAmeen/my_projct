{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e2851b",
   "metadata": {},
   "source": [
    "# Multimodal VQA Training\n",
    "\n",
    "**Goal**: Train a Visual Question Answering model combining text + vision\n",
    "\n",
    "This notebook walks through:\n",
    "1. Loading images and questions together\n",
    "2. Building a multimodal model (LSTM + ResNet CNN)\n",
    "3. Training with fusion strategies\n",
    "4. Evaluating performance\n",
    "5. Comparing with text-only baseline (47%)\n",
    "\n",
    "**Target**: 60-70% accuracy with vision!\n",
    "\n",
    "**Note**: Can run on Google Colab (GPU recommended for faster training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531ff86",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "**For Google Colab**: This cell will automatically mount your Drive and install packages.\n",
    "**For Local**: This cell will skip Colab-specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf844d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Running locally\n"
     ]
    }
   ],
   "source": [
    "# Google Colab setup (auto-detects environment)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install packages\n",
    "    print(\"Installing packages...\")\n",
    "    !pip install -q torch torchvision tqdm pyyaml scikit-learn pandas matplotlib seaborn Pillow\n",
    "    \n",
    "    # Set project path\n",
    "    import os\n",
    "    PROJECT_PATH = \"/content/drive/MyDrive/WOA7015 Advanced Machine Learning\"\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"  Running on Colab - Path: {PROJECT_PATH}\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Running locally\n",
    "    PROJECT_PATH = None\n",
    "    print(\"  Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad218ef4",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500d09b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m     project_root = Path().absolute().parent\n\u001b[32m     11\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "if 'PROJECT_PATH' in globals() and PROJECT_PATH:\n",
    "    project_root = Path(PROJECT_PATH)\n",
    "else:\n",
    "    project_root = Path().absolute().parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataset import create_multimodal_dataloaders\n",
    "from src.models.multimodal_model import create_multimodal_model\n",
    "from src.training.multimodal_trainer import MultimodalVQATrainer\n",
    "from src.evaluation.metrics import VQAMetrics, calculate_accuracy\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"  Imports successful\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0aa74",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6103398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "model_type='attention'  # Valid options: 'concat', 'attention', 'bilinear', 'cross_attention'\n",
    "# Update config for multimodal training\n",
    "config['training']['batch_size'] = 16  # Smaller batch for multimodal\n",
    "config['training']['num_epochs'] = 10\n",
    "config['training']['learning_rate'] = 1e-4  # Lower LR for vision features\n",
    "config['model']['vision_encoder'] = 'resnet50'\n",
    "config['model']['fusion_strategy'] = model_type  # Start with simplest\n",
    "print(\"\\nMultimodal training config:\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Vision encoder: {config['model']['vision_encoder']}\")\n",
    "print(f\"  Fusion strategy: {config['model']['fusion_strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beaea4d",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Loading the PathVQA dataset with **images** using the new `MultimodalVQADataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4fc929",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create multimodal dataloaders\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset_path = \u001b[43mproject_root\u001b[49m / \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m train_loader, val_loader, test_loader, vocab_size, num_classes, vocab, answer_to_idx = create_multimodal_dataloaders(\n\u001b[32m      4\u001b[39m     train_csv=\u001b[38;5;28mstr\u001b[39m(dataset_path / \u001b[33m'\u001b[39m\u001b[33mtrainrenamed.csv\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m      5\u001b[39m     test_csv=\u001b[38;5;28mstr\u001b[39m(dataset_path / \u001b[33m'\u001b[39m\u001b[33mtestrenamed.csv\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     image_size=\u001b[32m224\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Data loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'project_root' is not defined"
     ]
    }
   ],
   "source": [
    "# Create multimodal dataloaders\n",
    "dataset_path = project_root / 'data'\n",
    "train_loader, val_loader, test_loader, vocab_size, num_classes, vocab, answer_to_idx = create_multimodal_dataloaders(\n",
    "    train_csv=str(dataset_path / 'trainrenamed.csv'),\n",
    "    test_csv=str(dataset_path / 'testrenamed.csv'),\n",
    "    image_dir=str(dataset_path / 'train'),\n",
    "    answers_file=str(dataset_path / 'answers.txt'),  # This was missing!\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    val_split=0.1,\n",
    "    num_workers=0,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "print(f\"  Data loaded successfully\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Questions: {sample_batch['question'].shape}\")\n",
    "print(f\"  Images: {sample_batch['image'].shape}\")  # [batch, 3, 224, 224]\n",
    "print(f\"  Answers: {sample_batch['answer'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check answer vocabulary consistency\n",
    "print(\"Answer vocabulary debug:\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Max answer index in answer_to_idx: {max(answer_to_idx.values()) if answer_to_idx else 'N/A'}\")\n",
    "print(f\"  '<UNK>' token index: {answer_to_idx.get('<UNK>', 'Not present')}\")\n",
    "\n",
    "# Check a few samples from the training data\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch answer indices:\")\n",
    "print(f\"  Min answer index: {sample_batch['answer'].min().item()}\")\n",
    "print(f\"  Max answer index: {sample_batch['answer'].max().item()}\")\n",
    "print(f\"  Answer indices range should be 0 to {num_classes-1}\")\n",
    "\n",
    "# Verify all indices are within valid range\n",
    "max_answer_in_batch = sample_batch['answer'].max().item()\n",
    "if max_answer_in_batch >= num_classes:\n",
    "    print(f\"  ERROR: Found answer index {max_answer_in_batch} but only {num_classes} classes!\")\n",
    "    print(\"This will cause the IndexError during training\")\n",
    "else:\n",
    "    print(\"  All answer indices are within valid range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b14a02",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Images\n",
    "\n",
    "Let's verify the images are loading correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285397ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 4 sample images with their questions and answers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Get batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for i in range(4):\n",
    "    # Denormalize image for display\n",
    "    img = batch['image'][i].cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Get question and answer\n",
    "    question = batch['question'][i].cpu().numpy()\n",
    "    answer_idx = batch['answer'][i].item()\n",
    "    \n",
    "    # Decode question (first few words)\n",
    "    question_text = ' '.join([vocab.idx_to_word.get(idx, '<UNK>') \n",
    "                               for idx in question[:15] if idx > 0])\n",
    "    answer_text = vocab.idx_to_word.get(answer_idx, '<UNK>')\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Q: {question_text}...\\nA: {answer_text}\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  Images loading correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e186db2",
   "metadata": {},
   "source": [
    "## 5. Create Multimodal Model\n",
    "\n",
    "Creating a multimodal VQA model with:\n",
    "- **Vision Encoder**: ResNet50 CNN (extracts features from images)\n",
    "- **Text Encoder**: LSTM (processes questions)\n",
    "- **Fusion**: Concatenation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb05a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multimodal model\n",
    "\n",
    "model = create_multimodal_model(\n",
    "    model_type=config['model']['fusion_strategy'],  # Valid options: 'concat', 'attention', 'bilinear', 'cross_attention'\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=config['text']['embedding_dim'],\n",
    "    text_hidden_dim=config['model']['baseline']['hidden_dim'],\n",
    "    fusion_hidden_dim=config['model']['baseline']['hidden_dim'],\n",
    "    dropout=config['model']['baseline']['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"  Model created successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ebe48",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass\n",
    "\n",
    "Quick sanity check that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_questions = test_batch['question'].to(device)\n",
    "    test_images = test_batch['image'].to(device)\n",
    "    test_answers = test_batch['answer'].to(device)\n",
    "    \n",
    "    outputs = model(test_questions, test_images)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    accuracy = (predictions == test_answers).float().mean()\n",
    "    \n",
    "    print(f\"  Forward pass successful\")\n",
    "    print(f\"  Input questions shape: {test_questions.shape}\")\n",
    "    print(f\"  Input images shape: {test_images.shape}\")\n",
    "    print(f\"  Output shape: {outputs.shape}\")\n",
    "    print(f\"  Random accuracy: {accuracy.item():.4f} (should be ~0.002 for random)\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e2768",
   "metadata": {},
   "source": [
    "## 7. Setup Training\n",
    "\n",
    "Initialize the trainer with early stopping and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with enhanced history tracking\n",
    "experiment_name=f\"multimodal_{model_type}\"\n",
    "trainer = MultimodalVQATrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,  # Pass the full config dictionary\n",
    "    device=device,\n",
    "    checkpoint_dir=project_root / 'checkpoints',\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "\n",
    "# Initialize training history tracking if not present\n",
    "if not hasattr(trainer, 'train_losses'):\n",
    "    trainer.train_losses = []\n",
    "if not hasattr(trainer, 'val_losses'):\n",
    "    trainer.val_losses = []\n",
    "if not hasattr(trainer, 'train_accuracies'):\n",
    "    trainer.train_accuracies = []\n",
    "if not hasattr(trainer, 'val_accuracies'):\n",
    "    trainer.val_accuracies = []\n",
    "\n",
    "# Add history tracking method to trainer\n",
    "def track_epoch_history(self, train_loss, train_acc, val_loss, val_acc):\n",
    "    \"\"\"Track training history for plotting\"\"\"\n",
    "    self.train_losses.append(train_loss)\n",
    "    self.train_accuracies.append(train_acc)\n",
    "    self.val_losses.append(val_loss)\n",
    "    self.val_accuracies.append(val_acc)\n",
    "\n",
    "# Add method to trainer instance\n",
    "import types\n",
    "trainer.track_epoch_history = types.MethodType(track_epoch_history, trainer)\n",
    "\n",
    "print(f\"Trainer initialized with history tracking\")\n",
    "print(f\"  Checkpoint directory: {trainer.checkpoint_dir}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Early stopping patience: 5 epochs\")\n",
    "print(f\"  Training history will be automatically tracked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d5666",
   "metadata": {},
   "source": [
    "## 8. Train Model\n",
    "\n",
    "This will take some time (~1-2 hours on CPU, ~15-20 minutes on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8606dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop with guaranteed history tracking\n",
    "print(\"Starting Custom Training with History Tracking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = config['training']['num_epochs']\n",
    "patience = 5\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                             lr=config['training']['learning_rate'], \n",
    "                             weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    for batch in train_pbar:\n",
    "        questions = batch['question'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(questions, images)\n",
    "        loss = criterion(outputs, answers)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        train_correct += (predictions == answers).sum().item()\n",
    "        train_total += answers.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_acc = train_correct / train_total\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{current_acc:.4f}'\n",
    "        })\n",
    "    \n",
    "    train_loss_avg = train_loss / len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            questions = batch['question'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "            \n",
    "            outputs = model(questions, images)\n",
    "            loss = criterion(outputs, answers)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            val_correct += (predictions == answers).sum().item()\n",
    "            val_total += answers.size(0)\n",
    "    \n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Track history\n",
    "    trainer.track_epoch_history(train_loss_avg, train_acc, val_loss_avg, val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = trainer.checkpoint_dir / experiment_name / 'best_model.pth'\n",
    "        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss_avg,\n",
    "            'train_loss': train_loss_avg\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        trainer.best_val_acc = val_acc\n",
    "        print(f\"New best model saved! Val Acc: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "            break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "trainer.current_epoch = epoch\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"Total training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"Training epochs completed: {len(trainer.train_losses)}\")\n",
    "print(f\"Training history successfully tracked with {len(trainer.train_losses)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed1a87",
   "metadata": {},
   "source": [
    "## 9. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual training history from trainer\n",
    "if hasattr(trainer, 'train_losses') and len(trainer.train_losses) > 0:\n",
    "    # Use actual training history from trainer\n",
    "    epochs = list(range(1, len(trainer.train_losses) + 1))\n",
    "    train_losses = trainer.train_losses\n",
    "    val_losses = trainer.val_losses if hasattr(trainer, 'val_losses') else [0] * len(epochs)\n",
    "    train_accs = trainer.train_accuracies if hasattr(trainer, 'train_accuracies') else [0] * len(epochs)\n",
    "    val_accs = trainer.val_accuracies if hasattr(trainer, 'val_accuracies') else [0] * len(epochs)\n",
    "    \n",
    "    print(f\"Using actual training history from trainer\")\n",
    "    print(f\"  Training completed: {len(epochs)} epochs\")\n",
    "    print(f\"  Best validation accuracy: {max(val_accs):.4f} ({max(val_accs)*100:.2f}%)\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: Check if trainer has history method or attributes\n",
    "    if hasattr(trainer, 'get_history'):\n",
    "        history = trainer.get_history()\n",
    "        epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "        train_losses = history['train_loss']\n",
    "        val_losses = history['val_loss']\n",
    "        train_accs = history['train_acc']\n",
    "        val_accs = history['val_acc']\n",
    "        print(f\"Using training history from trainer.get_history()\")\n",
    "        \n",
    "    elif hasattr(trainer, 'history'):\n",
    "        history = trainer.history\n",
    "        epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "        train_losses = history['train_loss']\n",
    "        val_losses = history['val_loss'] \n",
    "        train_accs = history['train_acc']\n",
    "        val_accs = history['val_acc']\n",
    "        print(f\"Using training history from trainer.history\")\n",
    "        \n",
    "    else:\n",
    "        # No training history available - likely training hasn't been run yet\n",
    "        print(\"No training history found. Either:\")\n",
    "        print(\"  1. Training hasn't been completed yet\")\n",
    "        print(\"  2. Trainer doesn't store history\")\n",
    "        print(\"  3. Run the training cell first\")\n",
    "        \n",
    "        # Create placeholder for visualization\n",
    "        epochs = list(range(1, 11))\n",
    "        train_losses = [0] * 10\n",
    "        val_losses = [0] * 10\n",
    "        train_accs = [0] * 10\n",
    "        val_accs = [0] * 10\n",
    "        print(\"  Using placeholder data for plot structure\")\n",
    "\n",
    "# Create training history plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "if max(train_losses) > 0:  # Only plot if we have real data\n",
    "    axes[0].plot(epochs, train_losses, label='Train Loss', marker='o', color='blue')\n",
    "    axes[0].plot(epochs, val_losses, label='Val Loss', marker='s', color='orange')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'Training not completed\\nRun training cell first', \n",
    "                 ha='center', va='center', transform=axes[0].transAxes,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    axes[0].set_title('Training Loss (Pending)')\n",
    "\n",
    "# Plot accuracy\n",
    "if max(train_accs) > 0:  # Only plot if we have real data\n",
    "    axes[1].plot(epochs, train_accs, label='Train Accuracy', marker='o', color='blue')\n",
    "    axes[1].plot(epochs, val_accs, label='Val Accuracy', marker='s', color='orange')\n",
    "    axes[1].axhline(y=0.4736, color='r', linestyle='--', label='Text-only baseline (47.36%)')\n",
    "    \n",
    "    # Add best validation accuracy line\n",
    "    best_val_acc = max(val_accs)\n",
    "    if best_val_acc > 0:\n",
    "        axes[1].axhline(y=best_val_acc, color='g', linestyle=':', alpha=0.7, \n",
    "                       label=f'Best validation ({best_val_acc*100:.2f}%)')\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Print key achievements\n",
    "    if best_val_acc > 0.4736:\n",
    "        print(f\"SUCCESS: Beat text baseline by {(best_val_acc - 0.4736)*100:.2f} pp!\")\n",
    "    elif best_val_acc > 0.4125:\n",
    "        print(f\"PROGRESS: Improved over original multimodal by {(best_val_acc - 0.4125)*100:.2f} pp\")\n",
    "        \n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Training not completed\\nRun training cell first', \n",
    "                 ha='center', va='center', transform=axes[1].transAxes,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    axes[1].set_title('Training Accuracy (Pending)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot if we have real data\n",
    "if max(train_accs) > 0:\n",
    "    results_dir = project_root / 'results' / 'figures'\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(results_dir / 'multimodal_training_history.png', dpi=150)\n",
    "    print(f\"Training history plot saved to {results_dir / 'multimodal_training_history.png'}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print training summary if data is available\n",
    "if max(train_accs) > 0:\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Total epochs: {len(epochs)}\")\n",
    "    print(f\"  Final train accuracy: {train_accs[-1]*100:.2f}%\")\n",
    "    print(f\"  Final validation accuracy: {val_accs[-1]*100:.2f}%\")\n",
    "    print(f\"  Best validation accuracy: {max(val_accs)*100:.2f}%\")\n",
    "    print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ad5e4",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4912b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model checkpoint\n",
    "checkpoint_path = trainer.checkpoint_dir /\"multimodal_concat\" / 'best_model.pth'\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # Optionally update best_val_acc in trainer if needed for consistency\n",
    "    trainer.best_val_acc = checkpoint.get('val_acc', trainer.best_val_acc if hasattr(trainer, 'best_val_acc') else 0.0)\n",
    "    print(f\"  Loaded best model from: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"   Best model not found at: {checkpoint_path}\")\n",
    "    print(\"Using current model state for evaluation\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc, all_preds, all_labels = trainer.evaluate(test_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f3012",
   "metadata": {},
   "source": [
    "## 11. Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c16e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test samples and predictions\n",
    "model.eval()\n",
    "test_batch = next(iter(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    questions = test_batch['question'].to(device)\n",
    "    images = test_batch['image'].to(device)\n",
    "    true_answers = test_batch['answer'].to(device)\n",
    "    \n",
    "    outputs = model(questions, images)\n",
    "    pred_answers = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(9):\n",
    "    # Denormalize image\n",
    "    img = images[i].cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Get question\n",
    "    question = questions[i].cpu().numpy()\n",
    "    question_text = ' '.join([vocab.idx_to_word.get(idx, '<UNK>') \n",
    "                               for idx in question[:20] if idx > 0])\n",
    "    \n",
    "    # Get answers\n",
    "    true_ans = vocab.idx_to_word.get(true_answers[i].item(), '<UNK>')\n",
    "    pred_ans = vocab.idx_to_word.get(pred_answers[i].item(), '<UNK>')\n",
    "    \n",
    "    # Determine if correct\n",
    "    is_correct = true_answers[i].item() == pred_answers[i].item()\n",
    "    color = 'green' if is_correct else 'red'\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(\n",
    "        f\"Q: {question_text}...\\n\"\n",
    "        f\"True: {true_ans} | Pred: {pred_ans}\",\n",
    "        fontsize=9,\n",
    "        color=color\n",
    "    )\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'multimodal_predictions.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"  Predictions visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c611d",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb79d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'true_answer': all_labels,\n",
    "    'predicted_answer': all_preds,\n",
    "    'correct': (np.array(all_preds) == np.array(all_labels)).astype(int)\n",
    "})\n",
    "\n",
    "results_path = project_root / 'results' / 'predictions' / 'multimodal_concat_predictions.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"  Results saved to {results_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total predictions: {len(results_df)}\")\n",
    "print(f\"  Correct: {results_df['correct'].sum()}\")\n",
    "print(f\"  Incorrect: {len(results_df) - results_df['correct'].sum()}\")\n",
    "print(f\"  Accuracy: {results_df['correct'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405340e2",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    " **Achieved Goals:**\n",
    "-  Implemented multimodal VQA with vision + text\n",
    "-  Trained concatenation fusion model\n",
    "-  Compared with text-only baseline (47.36%)\n",
    "\n",
    " **Potential Improvements:**\n",
    "\n",
    "1. **Try different fusion strategies:**\n",
    "   - Attention fusion (better weighting)\n",
    "   - Bilinear fusion (richer interactions)\n",
    "   - Cross-modal attention (full co-attention)\n",
    "\n",
    "2. **Better vision encoder:**\n",
    "   - Use AttentionVisionEncoder with spatial attention\n",
    "   - Try different CNN backbones (ResNet101, EfficientNet)\n",
    "   - Fine-tune vision encoder instead of freezing\n",
    "\n",
    "3. **Data augmentation:**\n",
    "   - Stronger image augmentation\n",
    "   - Text augmentation (paraphrasing)\n",
    "\n",
    "4. **Hyperparameter tuning:**\n",
    "   - Learning rate scheduling\n",
    "   - Different batch sizes\n",
    "   - Gradient clipping\n",
    "\n",
    "5. **Pre-trained models:**\n",
    "   - Use CLIP or ViLT\n",
    "   - Fine-tune BERT for questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b719339",
   "metadata": {},
   "source": [
    "## 14. Improved Multimodal Model\n",
    "\n",
    "Let's implement an **improved version** with the following enhancements:\n",
    "1. **Trainable ResNet50** (instead of frozen)\n",
    "2. **Cross-modal attention fusion** (instead of simple concatenation)\n",
    "3. **Spatial attention** for vision features\n",
    "4. **Better regularization** and training strategies\n",
    "\n",
    "This should significantly improve the 41.25% → potentially 50%+ accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13952fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ImprovedMultimodalVQA(nn.Module):\n",
    "    \"\"\"Improved multimodal VQA with cross-modal attention fusion\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim=300, \n",
    "                 text_hidden_dim=512, fusion_hidden_dim=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Text encoder (same as before)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.text_lstm = nn.LSTM(embedding_dim, text_hidden_dim, \n",
    "                                batch_first=True, bidirectional=True)\n",
    "        self.text_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Vision encoder (now trainable!) - Fix deprecation warning\n",
    "        self.vision_encoder = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        # Remove final classification layer\n",
    "        self.vision_encoder = nn.Sequential(*list(self.vision_encoder.children())[:-2])\n",
    "        \n",
    "        # Add spatial attention for vision\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Cross-modal fusion with attention\n",
    "        self.vision_proj = nn.Linear(2048, fusion_hidden_dim)\n",
    "        self.text_proj = nn.Linear(text_hidden_dim * 2, fusion_hidden_dim)\n",
    "        \n",
    "        # Multi-head cross-attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=fusion_hidden_dim, \n",
    "            num_heads=8, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Final classifier with more regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_hidden_dim, fusion_hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 1.5),  # Stronger dropout\n",
    "            nn.Linear(fusion_hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Better weight initialization\"\"\"\n",
    "        for module in [self.text_embedding, self.vision_proj, self.text_proj, self.classifier]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, questions, images):\n",
    "        batch_size = questions.size(0)\n",
    "        \n",
    "        # Text processing\n",
    "        text_embedded = self.text_embedding(questions)\n",
    "        text_output, (text_hidden, _) = self.text_lstm(text_embedded)\n",
    "        \n",
    "        # Use final hidden state (both directions)\n",
    "        text_features = torch.cat([text_hidden[0], text_hidden[1]], dim=1)  # [batch, 1024]\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        \n",
    "        # Vision processing with spatial attention\n",
    "        vision_maps = self.vision_encoder(images)  # [batch, 2048, 7, 7]\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        attention_weights = self.spatial_attention(vision_maps)  # [batch, 1, 7, 7]\n",
    "        attended_vision = vision_maps * attention_weights  # Broadcast multiply\n",
    "        \n",
    "        # Global average pooling\n",
    "        vision_features = F.adaptive_avg_pool2d(attended_vision, 1).squeeze()  # [batch, 2048]\n",
    "        \n",
    "        # Project to same dimension\n",
    "        vision_proj = self.vision_proj(vision_features)  # [batch, 512]\n",
    "        text_proj = self.text_proj(text_features)        # [batch, 512]\n",
    "        \n",
    "        # Cross-modal attention: text queries vision\n",
    "        text_proj = text_proj.unsqueeze(1)  # [batch, 1, 512]\n",
    "        vision_proj = vision_proj.unsqueeze(1)  # [batch, 1, 512]\n",
    "        \n",
    "        # Attention: query=text, key=vision, value=vision\n",
    "        attended_features, attention_weights = self.cross_attention(\n",
    "            query=text_proj.transpose(0, 1),\n",
    "            key=vision_proj.transpose(0, 1),\n",
    "            value=vision_proj.transpose(0, 1)\n",
    "        )\n",
    "        \n",
    "        # Back to [batch, hidden_dim]\n",
    "        fused_features = attended_features.transpose(0, 1).squeeze(1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"Improved multimodal model class defined!\")\n",
    "print(\"\\nKey improvements:\")\n",
    "print(\"1. ResNet50 is now trainable (not frozen)\")\n",
    "print(\"2. Spatial attention for focusing on important image regions\") \n",
    "print(\"3. Cross-modal attention for better text-vision fusion\")\n",
    "print(\"4. Better weight initialization and stronger regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved multimodal model\n",
    "model = ImprovedMultimodalVQA(\n",
    "    vocab_size=len(vocab),\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=300,\n",
    "    text_hidden_dim=512,\n",
    "    fusion_hidden_dim=512,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "try:\n",
    "    # Get a small batch for testing\n",
    "    test_batch = next(iter(train_loader))\n",
    "    \n",
    "    # Handle different data loader formats\n",
    "    if isinstance(test_batch, dict):\n",
    "        # Dictionary format: {'question': tensor, 'image': tensor, 'answer': tensor}\n",
    "        test_questions = test_batch['question'].to(device)\n",
    "        test_images = test_batch['image'].to(device) \n",
    "        test_answers = test_batch['answer'].to(device)\n",
    "    elif isinstance(test_batch, (list, tuple)) and len(test_batch) == 3:\n",
    "        # List/tuple format: (questions, images, answers)\n",
    "        test_questions, test_images, test_answers = test_batch\n",
    "        test_questions = test_questions.to(device)\n",
    "        test_images = test_images.to(device)\n",
    "        test_answers = test_answers.to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected batch format: {type(test_batch)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_questions, test_images)\n",
    "\n",
    "    print(f\"Forward pass successful!\")\n",
    "    print(f\"Input shapes: questions={test_questions.shape}, images={test_images.shape}\")\n",
    "    print(f\"Output shape: {test_outputs.shape}\")\n",
    "    print(f\"Output range: [{test_outputs.min().item():.3f}, {test_outputs.max().item():.3f}]\")\n",
    "    print(f\"Batch format: {type(test_batch)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in forward pass: {e}\")\n",
    "    print(f\"Batch type: {type(test_batch)}\")\n",
    "    print(f\"Batch content: {test_batch if not isinstance(test_batch, (dict, list, tuple)) else 'batch data'}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e7e61",
   "metadata": {},
   "source": [
    "## 15. Enhanced Training Setup\n",
    "\n",
    "Now let's set up **enhanced training** with:\n",
    "- **Differential learning rates** (lower for pretrained vision, higher for new components)\n",
    "- **Label smoothing** to reduce overconfidence\n",
    "- **Better scheduling** and **gradient clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training configuration with differential learning rates\n",
    "import torch.optim as optim\n",
    "\n",
    "vision_params = []\n",
    "other_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'vision_encoder' in name:\n",
    "        vision_params.append(param)\n",
    "    else:\n",
    "        other_params.append(param)\n",
    "\n",
    "# Different learning rates for different components\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': vision_params, 'lr': 1e-5, 'weight_decay': 1e-4},    # Lower for pretrained\n",
    "    {'params': other_params, 'lr': 1e-3, 'weight_decay': 1e-4}     # Higher for new layers\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# Enhanced loss function with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(\"Enhanced training setup configured:\")\n",
    "print(f\"- Vision encoder parameters: {len(vision_params):,}\")\n",
    "print(f\"- Other parameters: {len(other_params):,}\")\n",
    "print(f\"- Vision encoder LR: 1e-5\")\n",
    "print(f\"- Other components LR: 1e-3\")\n",
    "print(f\"- Using label smoothing: 0.1\")\n",
    "print(f\"- Scheduler: CosineAnnealingWarmRestarts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d9d5a",
   "metadata": {},
   "source": [
    "## 16. Train Improved Model\n",
    "\n",
    "Let's train the improved model and see if we can beat the original 41.25% accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81866320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_history(model, train_loader, val_loader, optimizer, criterion, scheduler, \n",
    "                           device, epoch, total_epochs):\n",
    "    \"\"\"Enhanced training with comprehensive history tracking\"\"\"\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{total_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Handle dictionary format data loader\n",
    "        if isinstance(batch, dict):\n",
    "            questions = batch['question'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "        else:\n",
    "            # Handle tuple format (fallback)\n",
    "            questions, images, answers = batch\n",
    "            questions, images, answers = questions.to(device), images.to(device), answers.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(questions, images)\n",
    "        loss = criterion(outputs, answers)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(loss.item())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += answers.size(0)\n",
    "        train_correct += (predicted == answers).sum().item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            current_acc = 100. * train_correct / train_total\n",
    "            print(f\"Batch {batch_idx:3d}/{len(train_loader):3d} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | \"\n",
    "                  f\"Acc: {current_acc:.2f}%\")\n",
    "    \n",
    "    # Calculate epoch averages\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    train_accuracy = 100. * train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Handle dictionary format data loader\n",
    "            if isinstance(batch, dict):\n",
    "                questions = batch['question'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "            else:\n",
    "                # Handle tuple format (fallback)\n",
    "                questions, images, answers = batch\n",
    "                questions, images, answers = questions.to(device), images.to(device), answers.to(device)\n",
    "            \n",
    "            outputs = model(questions, images)\n",
    "            loss = criterion(outputs, answers)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += answers.size(0)\n",
    "            val_correct += (predicted == answers).sum().item()\n",
    "    \n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_accuracy = 100. * val_correct / val_total\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_accuracy:.2f}%\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {\n",
    "        'train_loss': avg_train_loss,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop with comprehensive history tracking\n",
    "EPOCHS = 15\n",
    "history = {\n",
    "    'train_losses': [],\n",
    "    'train_accuracies': [],\n",
    "    'val_losses': [],\n",
    "    'val_accuracies': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting enhanced multimodal training...\")\n",
    "print(f\"Training for {EPOCHS} epochs with differential learning rates\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # Train one epoch\n",
    "        epoch_results = train_epoch_with_history(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer, criterion, scheduler, \n",
    "            device, epoch, EPOCHS\n",
    "        )\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(epoch_results['train_loss'])\n",
    "        history['train_accuracies'].append(epoch_results['train_accuracy'])\n",
    "        history['val_losses'].append(epoch_results['val_loss'])\n",
    "        history['val_accuracies'].append(epoch_results['val_accuracy'])\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_results['val_accuracy'] > best_val_acc:\n",
    "            best_val_acc = epoch_results['val_accuracy']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = project_root / 'checkpoints' / 'multimodal_concat' / 'best_model.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af759dc",
   "metadata": {},
   "source": [
    "## 17. Compare Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of training history\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs_range = range(1, len(history['train_losses']) + 1)\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(epochs_range, history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs_range, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(epochs_range, history['train_accuracies'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(epochs_range, history['val_accuracies'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate over epochs (approximate)\n",
    "lr_values = [1e-3 * (0.5 ** (epoch // 5)) for epoch in epochs_range]\n",
    "ax3.semilogy(epochs_range, lr_values, 'g-', label='Learning Rate', linewidth=2)\n",
    "ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_ylabel('Learning Rate (log scale)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Training summary statistics\n",
    "final_stats = {\n",
    "    'Final Train Acc': f\"{history['train_accuracies'][-1]:.2f}%\",\n",
    "    'Final Val Acc': f\"{history['val_accuracies'][-1]:.2f}%\",\n",
    "    'Best Val Acc': f\"{max(history['val_accuracies']):.2f}%\",\n",
    "    'Final Train Loss': f\"{history['train_losses'][-1]:.4f}\",\n",
    "    'Final Val Loss': f\"{history['val_losses'][-1]:.4f}\",\n",
    "    'Total Epochs': len(history['train_losses'])\n",
    "}\n",
    "\n",
    "ax4.axis('off')\n",
    "stats_text = '\\n'.join([f\"{k}: {v}\" for k, v in final_stats.items()])\n",
    "ax4.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "ax4.set_title('Training Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"Enhanced Multimodal Model Training Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Training Accuracy: {history['train_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {history['val_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracies']):.2f}%\")\n",
    "print(f\"Total Training Epochs: {len(history['train_losses'])}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd46d9",
   "metadata": {},
   "source": [
    "## 18. Evaluate Improved Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fac7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Handle dictionary format data loader\n",
    "            if isinstance(batch, dict):\n",
    "                questions = batch['question'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "            else:\n",
    "                # Handle tuple format (fallback)\n",
    "                questions, images, answers = batch\n",
    "                questions, images, answers = questions.to(device), images.to(device), answers.to(device)\n",
    "            \n",
    "            outputs = model(questions, images)\n",
    "            loss = criterion(outputs, answers)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(answers.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'avg_loss': avg_loss,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "# Evaluate the improved model\n",
    "print(\"Evaluating improved multimodal model on test set...\")\n",
    "improved_results = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(\"\\nImproved Multimodal Model Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Test Accuracy: {improved_results['accuracy']:.4f} ({improved_results['accuracy']*100:.2f}%)\")\n",
    "print(f\"Test Precision: {improved_results['precision']:.4f}\")\n",
    "print(f\"Test Recall: {improved_results['recall']:.4f}\")\n",
    "print(f\"Test F1-Score: {improved_results['f1_score']:.4f}\")\n",
    "print(f\"Test Loss: {improved_results['avg_loss']:.4f}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load and compare with baseline if available\n",
    "baseline_path = project_root / 'results' / 'text_baseline_results.json'\n",
    "if baseline_path.exists():\n",
    "    import json\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    \n",
    "    baseline_acc = baseline_results.get('accuracy', 0.0)\n",
    "    improvement = improved_results['accuracy'] - baseline_acc\n",
    "    \n",
    "    print(f\"\\nComparison with Text Baseline:\")\n",
    "    print(f\"Baseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
    "    print(f\"Improved Model: {improved_results['accuracy']:.4f} ({improved_results['accuracy']*100:.2f}%)\")\n",
    "    print(f\"Improvement: {improvement:.4f} ({improvement*100:.2f}% points)\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"Performance improvement achieved!\")\n",
    "    else:\n",
    "        print(\"Performance did not improve - consider further tuning\")\n",
    "else:\n",
    "    print(\"Baseline results not found - run text baseline first for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65253f",
   "metadata": {},
   "source": [
    "## 19. Compare Answer Distributions and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save improved model results\n",
    "results_dir = project_root / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare results dictionary\n",
    "improved_model_results = {\n",
    "    'model_type': 'improved_multimodal',\n",
    "    'architecture': 'ResNet50 + LSTM + Cross-Modal Attention',\n",
    "    'test_metrics': {\n",
    "        'accuracy': improved_results['accuracy'],\n",
    "        'precision': improved_results['precision'],\n",
    "        'recall': improved_results['recall'],\n",
    "        'f1_score': improved_results['f1_score'],\n",
    "        'loss': improved_results['avg_loss']\n",
    "    },\n",
    "    'training_history': history,\n",
    "    'best_validation_accuracy': max(history['val_accuracies']) / 100.0,\n",
    "    'final_validation_accuracy': history['val_accuracies'][-1] / 100.0,\n",
    "    'model_parameters': {\n",
    "        'vocab_size': len(vocab),\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'embedding_dim': 300,\n",
    "        'text_hidden_dim': 512,\n",
    "        'fusion_hidden_dim': 512,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': len(history['train_losses']),\n",
    "        'vision_lr': 1e-5,\n",
    "        'other_lr': 1e-3,\n",
    "        'label_smoothing': 0.1,\n",
    "        'scheduler': 'CosineAnnealingWarmRestarts'\n",
    "    },\n",
    "    'improvements': [\n",
    "        'Trainable ResNet50 vision encoder',\n",
    "        'Spatial attention mechanism',\n",
    "        'Cross-modal attention fusion',\n",
    "        'Differential learning rates',\n",
    "        'Label smoothing',\n",
    "        'Gradient clipping',\n",
    "        'Enhanced regularization'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_file = results_dir / 'improved_multimodal_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(improved_model_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "if len(set(improved_results['targets'])) > 1:  # Check if we have multiple classes\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(improved_results['targets'], improved_results['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(NUM_CLASSES), \n",
    "                yticklabels=range(NUM_CLASSES))\n",
    "    plt.title('Improved Multimodal Model - Confusion Matrix')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    cm_path = results_dir / 'figures' / 'improved_multimodal_confusion_matrix.png'\n",
    "    cm_path.parent.mkdir(exist_ok=True)\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Confusion matrix saved to: {cm_path}\")\n",
    "\n",
    "# Performance comparison visualization if baseline exists\n",
    "baseline_path = project_root / 'results' / 'text_baseline_results.json'\n",
    "if baseline_path.exists():\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    \n",
    "    # Create comparison chart\n",
    "    models = ['Text Baseline', 'Improved Multimodal']\n",
    "    accuracies = [baseline_results['accuracy'], improved_results['accuracy']]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models, [acc*100 for acc in accuracies], \n",
    "                   color=['skyblue', 'lightcoral'], alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{acc*100:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, max(accuracies)*100 + 5)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add improvement annotation\n",
    "    improvement = (improved_results['accuracy'] - baseline_results['accuracy']) * 100\n",
    "    plt.annotate(f'Improvement: {improvement:+.2f}%', \n",
    "                xy=(1, accuracies[1]*100), xytext=(1.2, accuracies[1]*100 + 2),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                fontsize=12, fontweight='bold', color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison chart\n",
    "    comparison_path = results_dir / 'figures' / 'model_comparison.png'\n",
    "    comparison_path.parent.mkdir(exist_ok=True)\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Comparison chart saved to: {comparison_path}\")\n",
    "\n",
    "print(\"\\nImproved multimodal training completed successfully!\")\n",
    "print(\"All results, visualizations, and model checkpoints have been saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
