{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aa57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32592538",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b544518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = '..'  # Parent directory\n",
    "train_csv = os.path.join(data_dir, 'trainrenamed.csv')\n",
    "test_csv = os.path.join(data_dir, 'testrenamed.csv')\n",
    "answers_file = os.path.join(data_dir, 'answers.txt')\n",
    "image_dir = os.path.join(data_dir, 'train')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "# Load answers vocabulary\n",
    "with open(answers_file, 'r', encoding='utf-8') as f:\n",
    "    answer_vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df):,}\")\n",
    "print(f\"\\nAnswer vocabulary size: {len(answer_vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb103579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\n=== Training Data Sample ===\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac396a",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique images\n",
    "train_images = train_df['image'].nunique()\n",
    "test_images = test_df['image'].nunique()\n",
    "\n",
    "# Questions per image\n",
    "train_qa_per_image = len(train_df) / train_images\n",
    "test_qa_per_image = len(test_df) / test_images\n",
    "\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Images: {train_images:,}\")\n",
    "print(f\"  Q&A pairs: {len(train_df):,}\")\n",
    "print(f\"  Avg Q&A per image: {train_qa_per_image:.2f}\")\n",
    "\n",
    "print(f\"\\nTest:\")\n",
    "print(f\"  Images: {test_images:,}\")\n",
    "print(f\"  Q&A pairs: {len(test_df):,}\")\n",
    "print(f\"  Avg Q&A per image: {test_qa_per_image:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal unique images: {train_images + test_images:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Q&A pairs per image\n",
    "train_qa_counts = train_df['image'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_qa_counts.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Q&A pairs per image')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Q&A pairs per Image (Training)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(train_qa_counts.values, vert=True)\n",
    "plt.ylabel('Number of Q&A pairs')\n",
    "plt.title('Q&A pairs per Image - Box Plot')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/qa_per_image_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Min Q&A per image: {train_qa_counts.min()}\")\n",
    "print(f\"Max Q&A per image: {train_qa_counts.max()}\")\n",
    "print(f\"Median Q&A per image: {train_qa_counts.median():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfb8fc",
   "metadata": {},
   "source": [
    "## 3. Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question length statistics\n",
    "train_df['question_length'] = train_df['question'].str.split().str.len()\n",
    "test_df['question_length'] = test_df['question'].str.split().str.len()\n",
    "\n",
    "print(\"=== Question Length Statistics ===\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(train_df['question_length'].describe())\n",
    "print(f\"\\nTest:\")\n",
    "print(test_df['question_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot question length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_df['question_length'], bins=30, alpha=0.7, label='Train', edgecolor='black')\n",
    "plt.hist(test_df['question_length'], bins=30, alpha=0.7, label='Test', edgecolor='black')\n",
    "plt.xlabel('Question Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Question Length Distribution')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([train_df['question_length'], test_df['question_length']], \n",
    "            labels=['Train', 'Test'])\n",
    "plt.ylabel('Question Length (words)')\n",
    "plt.title('Question Length - Box Plot')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/question_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question type analysis (based on starting words)\n",
    "def get_question_type(question):\n",
    "    question_lower = question.lower().strip()\n",
    "    if question_lower.startswith('what'):\n",
    "        return 'What'\n",
    "    elif question_lower.startswith('is') or question_lower.startswith('are'):\n",
    "        return 'Yes/No'\n",
    "    elif question_lower.startswith('does') or question_lower.startswith('do'):\n",
    "        return 'Yes/No'\n",
    "    elif question_lower.startswith('where'):\n",
    "        return 'Where'\n",
    "    elif question_lower.startswith('how'):\n",
    "        return 'How'\n",
    "    elif question_lower.startswith('which'):\n",
    "        return 'Which'\n",
    "    elif question_lower.startswith('why'):\n",
    "        return 'Why'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "train_df['question_type'] = train_df['question'].apply(get_question_type)\n",
    "test_df['question_type'] = test_df['question'].apply(get_question_type)\n",
    "\n",
    "# Count question types\n",
    "train_qtype_counts = train_df['question_type'].value_counts()\n",
    "test_qtype_counts = test_df['question_type'].value_counts()\n",
    "\n",
    "print(\"=== Question Types ===\")\n",
    "print(\"\\nTraining:\")\n",
    "print(train_qtype_counts)\n",
    "print(f\"\\nPercentages:\")\n",
    "print(train_qtype_counts / len(train_df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot question types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training\n",
    "axes[0].pie(train_qtype_counts.values, labels=train_qtype_counts.index, \n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Question Types - Training')\n",
    "\n",
    "# Test\n",
    "axes[1].pie(test_qtype_counts.values, labels=test_qtype_counts.index, \n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Question Types - Test')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/question_types.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95262ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common question words\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize all questions\n",
    "all_questions = ' '.join(train_df['question'].values).lower()\n",
    "words = re.findall(r'\\b\\w+\\b', all_questions)\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Remove common stop words\n",
    "stop_words = {'the', 'a', 'an', 'is', 'are', 'of', 'in', 'to', 'and', 'or', 'with', 'this', 'that'}\n",
    "filtered_words = {word: count for word, count in word_counts.items() if word not in stop_words}\n",
    "\n",
    "# Top 20 words\n",
    "top_words = Counter(filtered_words).most_common(20)\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(words)), counts, color='steelblue')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Most Common Words in Questions (excluding stop words)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/common_question_words.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200432c2",
   "metadata": {},
   "source": [
    "## 4. Answer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad681e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer statistics\n",
    "train_unique_answers = train_df['answer'].nunique()\n",
    "test_unique_answers = test_df['answer'].nunique()\n",
    "\n",
    "print(\"=== Answer Statistics ===\")\n",
    "print(f\"\\nTraining unique answers: {train_unique_answers:,}\")\n",
    "print(f\"Test unique answers: {test_unique_answers:,}\")\n",
    "print(f\"Total unique answers in vocab: {len(answer_vocab):,}\")\n",
    "\n",
    "# Answer length\n",
    "train_df['answer_length'] = train_df['answer'].str.split().str.len()\n",
    "test_df['answer_length'] = test_df['answer'].str.split().str.len()\n",
    "\n",
    "print(\"\\n=== Answer Length Statistics ===\")\n",
    "print(\"\\nTraining:\")\n",
    "print(train_df['answer_length'].describe())\n",
    "print(\"\\nTest:\")\n",
    "print(test_df['answer_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify yes/no questions\n",
    "train_yes_no = train_df[train_df['answer'].isin(['yes', 'no'])]\n",
    "train_open_ended = train_df[~train_df['answer'].isin(['yes', 'no'])]\n",
    "\n",
    "print(f\"=== Closed vs Open-Ended Questions ===\")\n",
    "print(f\"\\nClosed-ended (yes/no): {len(train_yes_no):,} ({len(train_yes_no)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Open-ended: {len(train_open_ended):,} ({len(train_open_ended)/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Distribution of yes/no\n",
    "print(\"\\nYes/No distribution:\")\n",
    "print(train_df[train_df['answer'].isin(['yes', 'no'])]['answer'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af831653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 most common answers\n",
    "top_answers = train_df['answer'].value_counts().head(30)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(range(len(top_answers)), top_answers.values, color='coral')\n",
    "plt.yticks(range(len(top_answers)), top_answers.index, fontsize=10)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 30 Most Frequent Answers (Training Set)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/top_answers.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 answers:\")\n",
    "print(top_answers.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62418439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer distribution - cumulative\n",
    "answer_counts = train_df['answer'].value_counts()\n",
    "cumulative_coverage = np.cumsum(answer_counts.values) / len(train_df) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cumulative_coverage[:100])\n",
    "plt.xlabel('Number of Most Common Answers')\n",
    "plt.ylabel('Cumulative Coverage (%)')\n",
    "plt.title('Cumulative Answer Coverage')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='50% coverage')\n",
    "plt.axhline(y=80, color='g', linestyle='--', label='80% coverage')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(train_df['answer_length'], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Answer Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Answer Length Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/answer_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find how many answers cover 50% and 80% of data\n",
    "idx_50 = np.argmax(cumulative_coverage >= 50)\n",
    "idx_80 = np.argmax(cumulative_coverage >= 80)\n",
    "print(f\"\\nTop {idx_50} answers cover 50% of the data\")\n",
    "print(f\"Top {idx_80} answers cover 80% of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7a65f",
   "metadata": {},
   "source": [
    "## 5. Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random images and analyze their properties\n",
    "sample_images = train_df['image'].unique()[:100]  # Sample 100 images\n",
    "\n",
    "image_sizes = []\n",
    "image_aspects = []\n",
    "image_channels = []\n",
    "\n",
    "for img_name in sample_images:\n",
    "    img_path = os.path.join(image_dir, f\"{img_name}.png\" if not img_name.endswith('.png') else img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            image_sizes.append((width, height))\n",
    "            image_aspects.append(width / height)\n",
    "            image_channels.append(len(img.getbands()))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_name}: {e}\")\n",
    "\n",
    "widths, heights = zip(*image_sizes)\n",
    "\n",
    "print(\"=== Image Statistics (sample of 100) ===\")\n",
    "print(f\"\\nWidth - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.1f}\")\n",
    "print(f\"Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.1f}\")\n",
    "print(f\"Aspect Ratio - Min: {min(image_aspects):.2f}, Max: {max(image_aspects):.2f}, Mean: {np.mean(image_aspects):.2f}\")\n",
    "print(f\"Channels - {Counter(image_channels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image size distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(widths, heights, alpha=0.6)\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.title('Image Dimensions Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(image_aspects, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Aspect Ratio (width/height)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Aspect Ratio Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/image_properties.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429218d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images with their Q&A\n",
    "sample_imgs = train_df['image'].unique()[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_name in enumerate(sample_imgs):\n",
    "    # Load image\n",
    "    img_path = os.path.join(image_dir, f\"{img_name}.png\" if not img_name.endswith('.png') else img_name)\n",
    "    \n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        # Get one Q&A for this image\n",
    "        qa = train_df[train_df['image'] == img_name].iloc[0]\n",
    "        question = qa['question'][:50] + \"...\" if len(qa['question']) > 50 else qa['question']\n",
    "        answer = qa['answer'][:30] + \"...\" if len(qa['answer']) > 30 else qa['answer']\n",
    "        \n",
    "        title = f\"Q: {question}\\nA: {answer}\"\n",
    "        axes[idx].set_title(title, fontsize=9, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/sample_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89310cc",
   "metadata": {},
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ff4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"=== Duplicate Analysis ===\")\n",
    "print(f\"\\nDuplicate rows in training: {train_df.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows in test: {test_df.duplicated().sum()}\")\n",
    "\n",
    "# Check for same Q&A pairs\n",
    "train_qa_pairs = train_df[['question', 'answer']].drop_duplicates()\n",
    "test_qa_pairs = test_df[['question', 'answer']].drop_duplicates()\n",
    "\n",
    "print(f\"\\nUnique Q&A pairs in training: {len(train_qa_pairs):,}\")\n",
    "print(f\"Unique Q&A pairs in test: {len(test_qa_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all images exist\n",
    "missing_images = []\n",
    "sample_check = train_df['image'].unique()[:500]  # Check first 500\n",
    "\n",
    "for img_name in sample_check:\n",
    "    img_path = os.path.join(image_dir, f\"{img_name}.png\" if not img_name.endswith('.png') else img_name)\n",
    "    if not os.path.exists(img_path):\n",
    "        missing_images.append(img_name)\n",
    "\n",
    "print(f\"\\n=== Image Availability (checked {len(sample_check)} images) ===\")\n",
    "print(f\"Missing images: {len(missing_images)}\")\n",
    "if missing_images:\n",
    "    print(f\"Examples: {missing_images[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67514f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check answer vocabulary coverage\n",
    "train_answers_set = set(train_df['answer'].unique())\n",
    "vocab_set = set(answer_vocab)\n",
    "\n",
    "answers_not_in_vocab = train_answers_set - vocab_set\n",
    "vocab_not_in_answers = vocab_set - train_answers_set\n",
    "\n",
    "print(\"=== Answer Vocabulary Coverage ===\")\n",
    "print(f\"\\nAnswers in training not in vocab: {len(answers_not_in_vocab)}\")\n",
    "print(f\"Vocab entries not in training: {len(vocab_not_in_answers)}\")\n",
    "print(f\"\\nCoverage: {len(train_answers_set & vocab_set) / len(train_answers_set) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0f805",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e34672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS FOR MODEL DEVELOPMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   - Training: {len(train_df):,} Q&A pairs from {train_images:,} images\")\n",
    "print(f\"   - Test: {len(test_df):,} Q&A pairs from {test_images:,} images\")\n",
    "print(f\"   - Average {train_qa_per_image:.1f} Q&A pairs per image\")\n",
    "\n",
    "print(\"\\n2. QUESTION TYPES:\")\n",
    "yes_no_pct = len(train_yes_no) / len(train_df) * 100\n",
    "print(f\"   - Closed-ended (yes/no): {yes_no_pct:.1f}%\")\n",
    "print(f\"   - Open-ended: {100-yes_no_pct:.1f}%\")\n",
    "print(f\"   - Avg question length: {train_df['question_length'].mean():.1f} words\")\n",
    "\n",
    "print(\"\\n3. ANSWER CHARACTERISTICS:\")\n",
    "print(f\"   - Unique answers: {train_unique_answers:,}\")\n",
    "print(f\"   - Top {idx_50} answers cover 50% of data\")\n",
    "print(f\"   - Top {idx_80} answers cover 80% of data\")\n",
    "print(f\"   - Avg answer length: {train_df['answer_length'].mean():.1f} words\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(\"   - Use data augmentation for images (rotation, flip, color jitter)\")\n",
    "print(\"   - Consider separate models/heads for closed vs open-ended questions\")\n",
    "print(f\"   - Max sequence length for questions: {int(train_df['question_length'].quantile(0.95))} words (95th percentile)\")\n",
    "print(f\"   - Max sequence length for answers: {int(train_df['answer_length'].quantile(0.95))} words (95th percentile)\")\n",
    "print(\"   - Split validation by images (not Q&A pairs) to avoid leakage\")\n",
    "print(\"   - Normalize images to 224x224 for CNN models\")\n",
    "print(f\"   - Consider top {idx_80} answers for initial baseline (80% coverage)\")\n",
    "\n",
    "print(\"\\n5. EVALUATION STRATEGY:\")\n",
    "print(\"   - Closed-ended: Accuracy, F1-score\")\n",
    "print(\"   - Open-ended: BLEU, ROUGE, Exact Match, Semantic Similarity\")\n",
    "print(\"   - Overall: Weighted metrics based on question type distribution\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132a07e",
   "metadata": {},
   "source": [
    "## 8. Save Processed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b09d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key statistics for reference\n",
    "stats = {\n",
    "    'dataset': {\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'train_images': train_images,\n",
    "        'test_images': test_images,\n",
    "        'unique_answers': train_unique_answers\n",
    "    },\n",
    "    'questions': {\n",
    "        'avg_length': float(train_df['question_length'].mean()),\n",
    "        'max_length': int(train_df['question_length'].max()),\n",
    "        'median_length': float(train_df['question_length'].median()),\n",
    "        'recommended_max_length': int(train_df['question_length'].quantile(0.95))\n",
    "    },\n",
    "    'answers': {\n",
    "        'avg_length': float(train_df['answer_length'].mean()),\n",
    "        'max_length': int(train_df['answer_length'].max()),\n",
    "        'yes_no_percentage': float(yes_no_pct),\n",
    "        'recommended_max_length': int(train_df['answer_length'].quantile(0.95))\n",
    "    },\n",
    "    'question_types': train_qtype_counts.to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../results/dataset_statistics.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"Statistics saved to: ../results/dataset_statistics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51314ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/val split for reproducibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split by images\n",
    "unique_images = train_df['image'].unique()\n",
    "train_imgs, val_imgs = train_test_split(unique_images, test_size=0.15, random_state=42)\n",
    "\n",
    "train_split_df = train_df[train_df['image'].isin(train_imgs)]\n",
    "val_split_df = train_df[train_df['image'].isin(val_imgs)]\n",
    "\n",
    "# Save splits\n",
    "train_split_df.to_csv('../train_split.csv', index=False)\n",
    "val_split_df.to_csv('../val_split.csv', index=False)\n",
    "\n",
    "print(f\"\\nTrain/Val split saved:\")\n",
    "print(f\"  Train: {len(train_split_df):,} samples from {len(train_imgs):,} images\")\n",
    "print(f\"  Val: {len(val_split_df):,} samples from {len(val_imgs):,} images\")\n",
    "print(f\"\\nFiles saved: train_split.csv, val_split.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57106631",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided a comprehensive exploration of the PathVQA dataset:\n",
    "\n",
    "1. **Dataset Overview**: Analyzed the size and structure of training and test sets\n",
    "2. **Question Analysis**: Examined question types, lengths, and common patterns\n",
    "3. **Answer Analysis**: Studied answer distribution and identified closed vs open-ended questions\n",
    "4. **Image Analysis**: Investigated image properties and dimensions\n",
    "5. **Data Quality**: Checked for duplicates, missing values, and data integrity\n",
    "6. **Insights**: Generated actionable recommendations for model development\n",
    "7. **Preprocessing**: Created train/val split for reproducible experiments\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement baseline CNN + LSTM model\n",
    "- Implement Vision-Language Model (BLIP/ViLT)\n",
    "- Set up training and evaluation pipelines\n",
    "- Compare model performance on closed vs open-ended questions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
