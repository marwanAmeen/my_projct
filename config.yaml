# Configuration file for Med-VQA project

# Data paths
data:
  root_dir: "."
  train_images: "train"
  train_csv: "trainrenamed.csv"
  test_csv: "testrenamed.csv"
  answers_file: "answers.txt"
  val_split: 0.15  # 15% of training data for validation

# Image preprocessing
image:
  size: 224
  mean: [0.485, 0.456, 0.406]  # ImageNet mean
  std: [0.229, 0.224, 0.225]   # ImageNet std
  augmentation: true

# Text preprocessing
text:
  max_length: 64
  vocab_size: 10000
  embedding_dim: 300

# Model configurations
model:
  baseline:
    name: "baseline_cnn"
    image_encoder: "resnet50"
    pretrained: true
    hidden_dim: 512
    lstm_hidden: 256
    num_layers: 2
    dropout: 0.3
  
  vlm:
    name: "blip_vqa"
    model_name: "Salesforce/blip-vqa-base"
    freeze_vision: false
    freeze_text: false

# Training hyperparameters
training:
  batch_size: 32
  num_epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.0001
  optimizer: "adam"
  scheduler: "cosine"
  warmup_epochs: 5
  gradient_clip: 1.0
  early_stopping_patience: 10
  
  # Multi-GPU training
  num_gpus: 1
  distributed: false

# Evaluation
evaluation:
  metrics:
    - accuracy
    - f1_score
    - bleu
    - exact_match
  save_predictions: true

# Logging
logging:
  use_tensorboard: true
  use_wandb: false
  log_interval: 100
  save_interval: 1000
  
  wandb:
    project: "med-vqa"
    entity: null

# Paths
paths:
  checkpoints: "checkpoints"
  experiments: "experiments"
  results: "results"
  logs: "logs"

# Random seed for reproducibility
seed: 42
